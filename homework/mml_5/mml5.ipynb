{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0ZpnQDkKLTL"
   },
   "source": [
    "\n",
    "В данном домашнем задании вам необходимо обучить языковую модель (не более 1B параметров) решать примеры на сложение как можно более длинных чисел.\n",
    "\n",
    "**Ожидаемый результат.** Необходимо предоставить код, а также технический отчет, содержащий описание метода и используемых данных, оценку качества (точность сложения).\n",
    "\n",
    "**Пояснения:**\n",
    "\n",
    "1. Можно использовать любые предобученные модели, можно их файнтюнить, обучать с нуля, адаптировать любым другим способом или брать как есть. Главное, чтобы все использованные вами идеи, код или веса моделей были описаны в приложенном отчете со ссылкой на источник.\n",
    "\n",
    "2. Мы ожидаем, что ваш код принимает на вход два числа (в виде строк их десятичной записи) и выдает ответ в любом человекочитаемом виде. Однако, если ваша модель работает с входом в виде предложения на естественном языке, это тоже нормально, главное, чтобы это было описано в отчете.\n",
    "\n",
    "3. Можно оценить качество работы алгоритма, посчитав accuracy на случайных множествах чисел разной длины. Если вам кажется более подходящей другая метрика, мы примем ваше решение. Опишите вашу метрику и аргументируйте выбор в отчете.\n",
    "\n",
    "**Подсказка:** в качестве ориентира можете использовать следующий репозиторий (https://github.com/liutiedong/goat). В нем реализована сборка датасета для обучения и самообучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xs6tGTK-NOnv"
   },
   "source": [
    "### Разбаловка\n",
    "\n",
    "- [2 балла] Сбор датасета для обучения.  \n",
    "- [3 балла] Реализация скрипта модели.\n",
    "- [3 балла] Создание обученной модели с качеством (100% на числах длины < 10).\n",
    "- [2 баллов] Валидация модели.\n",
    "\n",
    "Для валидации можно использовать и готовую языковую модель без получения баллов за третий пункт."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4XrERP-wKwyN",
    "outputId": "47766e30-bee4-4194-bd44-d145f7512670"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Генерируем\n",
      "Слой 1: базовая выборка\n",
      "Слой 2: усиление средней сложности\n",
      "Слой 3: усиление высокой сложности\n",
      "Слой 4: усиление экстремальной сложности\n",
      "Слой 5: финальное усиление\n",
      "Генерация завершена за 0.4 сек\n",
      "Перемешиваем датасет\n",
      "\n",
      "Всего примеров: 68,100\n",
      "\n",
      "Распределение по сложности:\n",
      " 1-digit:    300 (  0.4%) \n",
      " 2-digit:    600 (  0.9%) \n",
      " 3-digit:   1200 (  1.8%) █\n",
      " 4-digit:   1800 (  2.6%) █\n",
      " 5-digit:   2400 (  3.5%) ██\n",
      " 6-digit:   3300 (  4.8%) ███\n",
      " 7-digit:   4200 (  6.2%) ████\n",
      " 8-digit:   5100 (  7.5%) █████\n",
      " 9-digit:   3900 (  5.7%) ███\n",
      "10-digit:   4800 (  7.0%) ████\n",
      "11-digit:   5700 (  8.4%) █████\n",
      "12-digit:   6900 ( 10.1%) ██████\n",
      "13-digit:   8100 ( 11.9%) ████████\n",
      "14-digit:   9300 ( 13.7%) █████████\n",
      "15-digit:  10500 ( 15.4%) ██████████\n",
      "\n",
      "Split:\n",
      "Train: 61,290 примеров\n",
      "Val: 6,810 примеров\n",
      "\n",
      "Сохраняем датасеты\n",
      "train_dataset.json (61,290 примеров)\n",
      "val_dataset.json (6,810 примеров)\n",
      "\n",
      "Примеры из датасета:\n",
      "4 + 646903815845 → 646903815849\n",
      "1254051952570 + 1380408128033 → 2634460080603\n",
      "199890455947158 + 482241063105826 → 682131519052984\n",
      "681352737127 + 77074623109746 → 77755975846873\n",
      "64705566 + 39109380232 → 39174085798\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from typing import List, Dict\n",
    "import time\n",
    "\n",
    "def generate_addition_dataset_full(\n",
    "    max_digits: int = 16,\n",
    "    examples_per_combination: int = 300\n",
    "    ) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Генерирует датасет для сложения с регулируемым числом примеров на каждую комбинацию\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Генерируем\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    dataset = []\n",
    "\n",
    "    print(\"Слой 1: базовая выборка\")\n",
    "    for i in range(1, max_digits):\n",
    "        for j in range(i, max_digits):\n",
    "            for _ in range(examples_per_combination):\n",
    "                num1 = random.randint(10**(i-1), 10**i - 1)\n",
    "                num2 = random.randint(10**(j-1), 10**j - 1)\n",
    "\n",
    "                answer = num1 + num2\n",
    "                dataset.append({\n",
    "                    \"input\": f\"{num1} + {num2}\",\n",
    "                    \"output\": f\"{num1} + {num2} = {answer}\",\n",
    "                    \"answer\": str(answer),\n",
    "                    \"num_length\": max(i, j)\n",
    "                })\n",
    "\n",
    "    print(\"Слой 2: усиление средней сложности\")\n",
    "    for i in range(3, 9):\n",
    "        for j in range(i, 9):\n",
    "            for _ in range(examples_per_combination):\n",
    "                num1 = random.randint(10**(i-1), 10**i - 1)\n",
    "                num2 = random.randint(10**(j-1), 10**j - 1)\n",
    "\n",
    "                answer = num1 + num2\n",
    "                dataset.append({\n",
    "                    \"input\": f\"{num1} + {num2}\",\n",
    "                    \"output\": f\"{num1} + {num2} = {answer}\",\n",
    "                    \"answer\": str(answer),\n",
    "                    \"num_length\": max(i, j)\n",
    "                })\n",
    "\n",
    "    print(\"Слой 3: усиление высокой сложности\")\n",
    "    for i in range(6, max_digits):\n",
    "        for j in range(i, max_digits):\n",
    "            for _ in range(examples_per_combination):\n",
    "                num1 = random.randint(10**(i-1), 10**i - 1)\n",
    "                num2 = random.randint(10**(j-1), 10**j - 1)\n",
    "\n",
    "                answer = num1 + num2\n",
    "                dataset.append({\n",
    "                    \"input\": f\"{num1} + {num2}\",\n",
    "                    \"output\": f\"{num1} + {num2} = {answer}\",\n",
    "                    \"answer\": str(answer),\n",
    "                    \"num_length\": max(i, j)\n",
    "                })\n",
    "\n",
    "    print(\"Слой 4: усиление экстремальной сложности\")\n",
    "    for i in range(10, max_digits):\n",
    "        for j in range(i, max_digits):\n",
    "            for _ in range(examples_per_combination):\n",
    "                num1 = random.randint(10**(i-1), 10**i - 1)\n",
    "                num2 = random.randint(10**(j-1), 10**j - 1)\n",
    "\n",
    "                answer = num1 + num2\n",
    "                dataset.append({\n",
    "                    \"input\": f\"{num1} + {num2}\",\n",
    "                    \"output\": f\"{num1} + {num2} = {answer}\",\n",
    "                    \"answer\": str(answer),\n",
    "                    \"num_length\": max(i, j)\n",
    "                })\n",
    "\n",
    "    print(\"Слой 5: финальное усиление\")\n",
    "    for i in range(12, max_digits):\n",
    "        for j in range(i, max_digits):\n",
    "            for _ in range(examples_per_combination):\n",
    "                num1 = random.randint(10**(i-1), 10**i - 1)\n",
    "                num2 = random.randint(10**(j-1), 10**j - 1)\n",
    "\n",
    "                answer = num1 + num2\n",
    "                dataset.append({\n",
    "                    \"input\": f\"{num1} + {num2}\",\n",
    "                    \"output\": f\"{num1} + {num2} = {answer}\",\n",
    "                    \"answer\": str(answer),\n",
    "                    \"num_length\": max(i, j)\n",
    "                })\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Генерация завершена за {elapsed:.1f} сек\")\n",
    "\n",
    "    print(\"Перемешиваем датасет\")\n",
    "    random.shuffle(dataset)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "random.seed(42)\n",
    "dataset = generate_addition_dataset_full(max_digits=16, examples_per_combination=300)\n",
    "print(f\"\\nВсего примеров: {len(dataset):,}\")\n",
    "\n",
    "by_length = {}\n",
    "for ex in dataset:\n",
    "    length = ex['num_length']\n",
    "    by_length[length] = by_length.get(length, 0) + 1\n",
    "\n",
    "print(\"\\nРаспределение по сложности:\")\n",
    "for length in sorted(by_length.keys()):\n",
    "    count = by_length[length]\n",
    "    percent = 100 * count / len(dataset)\n",
    "    bar = \"█\" * (count // 1000)\n",
    "    print(f\"{length:2d}-digit: {count:6d} ({percent:5.1f}%) {bar}\")\n",
    "\n",
    "train_size = int(len(dataset) * 0.9)\n",
    "train_dataset = dataset[:train_size]\n",
    "val_dataset = dataset[train_size:]\n",
    "\n",
    "print(f\"\\nSplit:\")\n",
    "print(f\"Train: {len(train_dataset):,} примеров\")\n",
    "print(f\"Val: {len(val_dataset):,} примеров\")\n",
    "\n",
    "print(f\"\\nСохраняем датасеты\")\n",
    "with open('train_dataset.json', 'w') as f:\n",
    "    json.dump(train_dataset, f)\n",
    "print(f\"train_dataset.json ({len(train_dataset):,} примеров)\")\n",
    "\n",
    "with open('val_dataset.json', 'w') as f:\n",
    "    json.dump(val_dataset, f)\n",
    "print(f\"val_dataset.json ({len(val_dataset):,} примеров)\")\n",
    "\n",
    "print(\"\\nПримеры из датасета:\")\n",
    "for i in range(5):\n",
    "    ex = train_dataset[i]\n",
    "    print(f\"{ex['input']} → {ex['answer']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514,
     "referenced_widgets": [
      "f42dd3f18f454948bd78eca6dd9df041",
      "b7731180c57f47c6b86e57b3cb8b5c99",
      "9542c432345342fdbf086dcb3e4bcca4",
      "86e4527f4f38477ba04e3ae173bbe7b7",
      "fdf81d9a9bd4481783208ba58e91fa8c",
      "ee66995acb24495cb4af3659007c4002",
      "f0f04c480c4b454bb6e39aaeb0f63c24",
      "c657cbf352ab469dae61eb76026a8e96",
      "83f89f760c8e41968bccb6aa20e000c3",
      "9fe81edf8c6d4a38b72721a6c80d7138",
      "0326c29cbddc4e408c31cab42ea378e1",
      "69468a7871d64e3cbd3787155e2497d0",
      "4c8fdb61fad84f5eb1347dda1ed1f9a9",
      "efe7e994a8814fbbb7eab86d867aa989",
      "c5d1d270139940a1a0f69290bed8eebb",
      "ed2049cea9be480da3572047f71a9d6e",
      "8ce1c512db654b46b7f77079b84e94a0",
      "39c366f3c5194e2abc03067fc1d5aa9b",
      "4d88e7bcd49842ceb2749d14546fbe27",
      "398c4ae944df43bb8c7f85bec57ce63f",
      "b1e24da9cd344127a8ffb1ceb0c5fafa",
      "f4003bda848c495da9e8b59c933bfc22"
     ]
    },
    "id": "d6u-b1Y9qitn",
    "outputId": "c4eccb54-9dd3-45c7-e6ad-026d0097c301"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "Device: cuda\n",
      "Загружаем модель и токенизатор\n",
      "trainable params: 1,126,400 || all params: 1,101,174,784 || trainable%: 0.1023\n",
      "Загружаем датасеты\n",
      "Токенизируем датасеты\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f42dd3f18f454948bd78eca6dd9df041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69468a7871d64e3cbd3787155e2497d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начинаем обучение\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 31:08, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.025700</td>\n",
       "      <td>1.020941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.012400</td>\n",
       "      <td>0.996266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение завершено\n",
      "Модель сохранена в ./arithmetic_model\n",
      "Модель сохранена ./arithmetic_model\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "BASE_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "OUTPUT_DIR = \"./arithmetic_model\"\n",
    "BATCH_SIZE = 8\n",
    "MICRO_BATCH_SIZE = 2\n",
    "NUM_EPOCHS = 2\n",
    "LEARNING_RATE = 2e-4\n",
    "CUTOFF_LEN = 128\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "print(\"Загружаем модель и токенизатор\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"Загружаем датасеты\")\n",
    "with open('train_dataset.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "with open('val_dataset.json', 'r') as f:\n",
    "    val_data = json.load(f)\n",
    "\n",
    "def create_simple_datasets(train_data, val_data, max_examples=5000):\n",
    "    \"\"\"Создаем уменьшенные датасеты для быстрого обучения\"\"\"\n",
    "    simple_train = [ex for ex in train_data if ex['num_length'] <= 8][:max_examples]\n",
    "    simple_val = [ex for ex in val_data if ex['num_length'] <= 8][:1000]\n",
    "\n",
    "    train_dataset = Dataset.from_dict({\n",
    "        'input': [ex['input'] for ex in simple_train],\n",
    "        'output': [ex['answer'] for ex in simple_train],\n",
    "    })\n",
    "    val_dataset = Dataset.from_dict({\n",
    "        'input': [ex['input'] for ex in simple_val],\n",
    "        'output': [ex['answer'] for ex in simple_val],\n",
    "    })\n",
    "\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "train_dataset, val_dataset = create_simple_datasets(train_data, val_data)\n",
    "\n",
    "def generate_prompt(input_text: str, output_text: str = None) -> str:\n",
    "    if output_text:\n",
    "        return f\"Calculate: {input_text}\\nAnswer: {output_text}\"\n",
    "    else:\n",
    "        return f\"Calculate: {input_text}\\nAnswer:\"\n",
    "\n",
    "def tokenize_function(example):\n",
    "    full_text = generate_prompt(example['input'], example['output'])\n",
    "    tokenized = tokenizer(\n",
    "        full_text,\n",
    "        max_length=CUTOFF_LEN,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    instruction_text = generate_prompt(example['input'])\n",
    "    instruction_tokens = tokenizer(\n",
    "        instruction_text,\n",
    "        max_length=CUTOFF_LEN,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    instruction_len = len(instruction_tokens['input_ids'])\n",
    "    labels = tokenized['input_ids'].copy()\n",
    "    labels[:instruction_len] = [-100] * instruction_len\n",
    "\n",
    "    tokenized['labels'] = labels\n",
    "    return tokenized\n",
    "\n",
    "print(\"Токенизируем датасеты\")\n",
    "train_dataset = train_dataset.map(tokenize_function, remove_columns=['input', 'output'])\n",
    "val_dataset = val_dataset.map(tokenize_function, remove_columns=['input', 'output'])\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=MICRO_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=BATCH_SIZE // MICRO_BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    seed=42,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "\n",
    "print(\"Начинаем обучение\")\n",
    "trainer.train()\n",
    "print(\"Обучение завершено\")\n",
    "\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"Модель сохранена в {OUTPUT_DIR}\")\n",
    "\n",
    "import shutil\n",
    "shutil.copytree(OUTPUT_DIR, f'/content/drive/MyDrive/{OUTPUT_DIR}', dirs_exist_ok=True)\n",
    "print(f\"Модель сохранена {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QZ5vVD4pilh8",
    "outputId": "2a95b40f-a4e0-4081-ebd3-6591c57c2602"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Запускаем валидацию\n",
      "Инициализация модели\n",
      "Загружаем модель\n",
      "Загружаем val_dataset.json\n",
      "Тестируем на 200 примерах\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Валидация: 100%|██████████| 200/200 [02:46<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "РЕЗУЛЬТАТЫ ВАЛИДАЦИИ:\n",
      "Общая точность: 42.00% (84/200)\n",
      "\n",
      "Точность по длине чисел:\n",
      "Длина 1: 100.00% (2/2)\n",
      "Длина 2: 85.71% (6/7)\n",
      "Длина 3: 81.82% (9/11)\n",
      "Длина 4: 90.00% (9/10)\n",
      "Длина 5: 45.00% (9/20)\n",
      "Длина 6: 57.69% (15/26)\n",
      "Длина 7: 54.55% (12/22)\n",
      "Длина 8: 58.82% (20/34)\n",
      "Длина 9: 8.00% (2/25)\n",
      "Длина 10: 0.00% (0/43)\n",
      "ТЕСТ НА ПРОСТЫХ ПРИМЕРАХ:\n",
      "Загружаем модель\n",
      "12 + 34 = 46 True\n",
      "5 + 7 = 12 True\n",
      "100 + 200 = 300 True\n",
      "999 + 1 = 1099 False (ожидалось 1000)\n",
      "25 + 75 = 100 True\n",
      "8 + 15 = 23 True\n",
      "Точность на простых примерах: 83.3%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "class ArithmeticModel:\n",
    "    def __init__(self, model_path):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Загружаем модель\")\n",
    "\n",
    "        try:\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "                device_map=\"auto\",\n",
    "                torch_dtype=torch.float16,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "\n",
    "            self.model = PeftModel.from_pretrained(self.model, model_path)\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "            self.model.eval()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка при загрузке модели: {e}\")\n",
    "\n",
    "    def generate_prompt(self, input_text):\n",
    "        return f\"Calculate: {input_text}\\nAnswer:\"\n",
    "\n",
    "    def add_numbers(self, a, b):\n",
    "        try:\n",
    "            input_text = f\"{a} + {b}\"\n",
    "            prompt = self.generate_prompt(input_text)\n",
    "\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=20,\n",
    "                    temperature=0.1,\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "\n",
    "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "            if \"Answer:\" in response:\n",
    "                answer_part = response.split(\"Answer:\")[1].strip()\n",
    "                numbers = re.findall(r'\\d+', answer_part)\n",
    "                if numbers:\n",
    "                    return numbers[0]\n",
    "\n",
    "            return \"0\"\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка при генерации: {e}\")\n",
    "            return \"0\"\n",
    "\n",
    "def validate_model(\n",
    "    model_path=\"./arithmetic_model\",\n",
    "    test_data_path=\"val_dataset.json\",\n",
    "    max_tests=200\n",
    "    ):\n",
    "    \"\"\"Валидация модели на тестовых данных\"\"\"\n",
    "\n",
    "    print(\"Инициализация модели\")\n",
    "    model = ArithmeticModel(model_path)\n",
    "\n",
    "    print(f\"Загружаем {test_data_path}\")\n",
    "    with open(test_data_path, 'r') as f:\n",
    "        test_data = json.load(f)\n",
    "\n",
    "    test_subset = [ex for ex in test_data if ex['num_length'] <= 10][:max_tests]\n",
    "    print(f\"Тестируем на {len(test_subset)} примерах\")\n",
    "\n",
    "    correct = 0\n",
    "    results_by_length = {}\n",
    "\n",
    "    for example in tqdm(test_subset, desc=\"Валидация\"):\n",
    "        input_text = example['input']\n",
    "        expected = example['answer']\n",
    "\n",
    "        numbers = re.findall(r'\\d+', input_text)\n",
    "        if len(numbers) >= 2:\n",
    "            a, b = numbers[0], numbers[1]\n",
    "            predicted = model.add_numbers(a, b)\n",
    "\n",
    "            num_length = max(len(a), len(b))\n",
    "            if num_length not in results_by_length:\n",
    "                results_by_length[num_length] = {'correct': 0, 'total': 0}\n",
    "\n",
    "            results_by_length[num_length]['total'] += 1\n",
    "            if predicted == expected:\n",
    "                correct += 1\n",
    "                results_by_length[num_length]['correct'] += 1\n",
    "\n",
    "    accuracy = correct / len(test_subset) * 100\n",
    "\n",
    "    print(f\"РЕЗУЛЬТАТЫ ВАЛИДАЦИИ:\")\n",
    "    print(f\"Общая точность: {accuracy:.2f}% ({correct}/{len(test_subset)})\")\n",
    "\n",
    "    print(f\"\\nТочность по длине чисел:\")\n",
    "    for length in sorted(results_by_length.keys()):\n",
    "        stats = results_by_length[length]\n",
    "        acc = stats['correct'] / stats['total'] * 100\n",
    "        print(f\"Длина {length}: {acc:.2f}% ({stats['correct']}/{stats['total']})\")\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "print(\"Запускаем валидацию\")\n",
    "accuracy = validate_model(\"./arithmetic_model\", \"val_dataset.json\")\n",
    "\n",
    "def quick_test():\n",
    "    print(\"ТЕСТ НА ПРОСТЫХ ПРИМЕРАХ:\")\n",
    "\n",
    "    model = ArithmeticModel(\"./arithmetic_model\")\n",
    "\n",
    "    test_cases = [\n",
    "        (\"12\", \"34\"),\n",
    "        (\"5\", \"7\"),\n",
    "        (\"100\", \"200\"),\n",
    "        (\"999\", \"1\"),\n",
    "        (\"25\", \"75\"),\n",
    "        (\"8\", \"15\"),\n",
    "    ]\n",
    "\n",
    "    correct = 0\n",
    "    for a, b in test_cases:\n",
    "        predicted = model.add_numbers(a, b)\n",
    "        expected = str(int(a) + int(b))\n",
    "        status = \"True\" if predicted == expected else f\"False (ожидалось {expected})\"\n",
    "        if predicted == expected:\n",
    "            correct += 1\n",
    "        print(f\"{a} + {b} = {predicted} {status}\")\n",
    "\n",
    "    print(f\"Точность на простых примерах: {correct/len(test_cases)*100:.1f}%\")\n",
    "\n",
    "quick_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "7cb06b56f6cd490d9b04a8da4c605a52",
      "81c9cd56f04b4cd98e4c55f97707d6cf",
      "63d5e430138b4bc6b2711c481d7ca695",
      "3253568efc8f4392ada9836a995ae69f",
      "048f3f11ef9d4dcebeac1ee21b79c580",
      "e224eb0c5aa04e249dd198d07201b0cf",
      "7931ba48b4954fdd8399902f05931b46",
      "da8a30a2f53a4fe28e2823f1a2a2387d",
      "8275768f1bce4e5cb1837e81b9d7ae51",
      "d6ea4c27c45146e48e29951e4df4ba3f",
      "10992a940088473cbfaed1d3eb60f8d7",
      "79045b0303ee49d1ba02c03675020f69",
      "990df4e6e31e493b843120dc090489df",
      "97c69425bf19477d920404274de6214e",
      "4b37a01227264c32acb901e73d4f890e",
      "6330647b75b24232affc462bea34667c",
      "323a6a9a6d3b4018b828fbd20b1fdd5d",
      "bb16107a000c4995b81d340ca15f069a",
      "90f177514d4f4b78834669243f825c20",
      "6059d83da4a54db5a6674a1b5647caff",
      "e227422d15d246d6b40ff186218bce93",
      "d9a2c13f218a4b09a777a5b3e8df7a49"
     ]
    },
    "id": "6FVlV6OTMzCo",
    "outputId": "a35a3ac0-70c5-4c1b-c8ec-0dfb382293da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Загружаем модель и токенизатор\n",
      "trainable params: 4,505,600 || all params: 1,104,553,984 || trainable%: 0.4079\n",
      "Загружаем датасеты\n",
      "Итоговый размер датасета:\n",
      "Train: 14000 примеров\n",
      "Val: 2600 примеров\n",
      "\n",
      "Распределение по длинам в train:\n",
      "  Длина 1: 86 примеров (0.6%)\n",
      "  Длина 2: 316 примеров (2.3%)\n",
      "  Длина 3: 466 примеров (3.3%)\n",
      "  Длина 4: 650 примеров (4.6%)\n",
      "  Длина 5: 863 примеров (6.2%)\n",
      "  Длина 6: 1102 примеров (7.9%)\n",
      "  Длина 7: 1379 примеров (9.8%)\n",
      "  Длина 8: 2344 примеров (16.7%)\n",
      "  Длина 9: 1683 примеров (12.0%)\n",
      "  Длина 10: 2111 примеров (15.1%)\n",
      "  Длина 11: 1390 примеров (9.9%)\n",
      "  Длина 12: 1610 примеров (11.5%)\n",
      "Токенизируем датасеты\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cb06b56f6cd490d9b04a8da4c605a52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79045b0303ee49d1ba02c03675020f69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "НАЧИНАЕМ ОБУЧЕНИЕ\n",
      "Модель: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "Эпохи: 3\n",
      "Размер train: 14000\n",
      "Размер validation: 2600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2625' max='2625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2625/2625 1:30:55, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.849600</td>\n",
       "      <td>0.883667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.814500</td>\n",
       "      <td>0.841895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.798100</td>\n",
       "      <td>0.832400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.805800</td>\n",
       "      <td>0.829155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.782400</td>\n",
       "      <td>0.824738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение завершено\n",
      "Модель сохранена в ./arithmetic_model_deep\n",
      "Mounted at /content/drive\n",
      "Модель сохранена ./arithmetic_model_deep\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "BASE_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "OUTPUT_DIR = \"./arithmetic_model_deep\"\n",
    "BATCH_SIZE = 16\n",
    "MICRO_BATCH_SIZE = 4\n",
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 1e-4\n",
    "CUTOFF_LEN = 128\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "print(\"Загружаем модель и токенизатор\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"Загружаем датасеты\")\n",
    "with open('train_dataset.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "with open('val_dataset.json', 'r') as f:\n",
    "    val_data = json.load(f)\n",
    "\n",
    "def create_balanced_datasets(train_data, val_data):\n",
    "    \"\"\"Создаем сбалансированный датасет с акцентом на сложные случаи\"\"\"\n",
    "\n",
    "    easy_train = [ex for ex in train_data if ex['num_length'] <= 7][:4000]\n",
    "    easy_val = [ex for ex in val_data if ex['num_length'] <= 7][:800]\n",
    "\n",
    "    medium_train = [ex for ex in train_data if 8 <= ex['num_length'] <= 10][:6000]\n",
    "    medium_val = [ex for ex in val_data if 8 <= ex['num_length'] <= 10][:1200]\n",
    "\n",
    "    hard_train = [ex for ex in train_data if 11 <= ex['num_length'] <= 12][:3000]\n",
    "    hard_val = [ex for ex in val_data if 11 <= ex['num_length'] <= 12][:600]\n",
    "\n",
    "    carry_over_cases = []\n",
    "    for _ in range(1000):\n",
    "        nines_count = random.randint(2, 8)\n",
    "        num1 = int('9' * nines_count)\n",
    "        num2 = random.randint(1, 100)\n",
    "        answer = num1 + num2\n",
    "        carry_over_cases.append({\n",
    "            \"input\": f\"{num1} + {num2}\",\n",
    "            \"output\": f\"{num1} + {num2} = {answer}\",\n",
    "            \"answer\": str(answer),\n",
    "            \"num_length\": len(str(num1))\n",
    "        })\n",
    "\n",
    "    balanced_train = easy_train + medium_train + hard_train + carry_over_cases\n",
    "    balanced_val = easy_val + medium_val + hard_val\n",
    "\n",
    "    random.shuffle(balanced_train)\n",
    "    random.shuffle(balanced_val)\n",
    "\n",
    "    print(f\"Итоговый размер датасета:\")\n",
    "    print(f\"Train: {len(balanced_train)} примеров\")\n",
    "    print(f\"Val: {len(balanced_val)} примеров\")\n",
    "\n",
    "    length_dist_train = {}\n",
    "    for ex in balanced_train:\n",
    "        length = ex['num_length']\n",
    "        length_dist_train[length] = length_dist_train.get(length, 0) + 1\n",
    "\n",
    "    print(\"\\nРаспределение по длинам в train:\")\n",
    "    for length in sorted(length_dist_train.keys()):\n",
    "        count = length_dist_train[length]\n",
    "        percent = count / len(balanced_train) * 100\n",
    "        print(f\"  Длина {length}: {count} примеров ({percent:.1f}%)\")\n",
    "\n",
    "    return balanced_train, balanced_val\n",
    "\n",
    "train_data_balanced, val_data_balanced = create_balanced_datasets(train_data, val_data)\n",
    "\n",
    "train_dataset = Dataset.from_dict({\n",
    "    'input': [ex['input'] for ex in train_data_balanced],\n",
    "    'output': [ex['answer'] for ex in train_data_balanced],\n",
    "})\n",
    "val_dataset = Dataset.from_dict({\n",
    "    'input': [ex['input'] for ex in val_data_balanced],\n",
    "    'output': [ex['answer'] for ex in val_data_balanced],\n",
    "})\n",
    "\n",
    "def generate_prompt(input_text: str, output_text: str = None) -> str:\n",
    "    if output_text:\n",
    "        return f\"\"\"### Arithmetic Problem:\n",
    "Calculate the sum: {input_text}\n",
    "\n",
    "### Solution:\n",
    "{output_text}\"\"\"\n",
    "    else:\n",
    "        return f\"\"\"### Arithmetic Problem:\n",
    "Calculate the sum: {input_text}\n",
    "\n",
    "### Solution:\n",
    "\"\"\"\n",
    "\n",
    "def tokenize_function(example):\n",
    "    full_text = generate_prompt(example['input'], example['output'])\n",
    "    tokenized = tokenizer(\n",
    "        full_text,\n",
    "        max_length=CUTOFF_LEN,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    instruction_text = generate_prompt(example['input'])\n",
    "    instruction_tokens = tokenizer(\n",
    "        instruction_text,\n",
    "        max_length=CUTOFF_LEN,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    instruction_len = len(instruction_tokens['input_ids'])\n",
    "    labels = tokenized['input_ids'].copy()\n",
    "    labels[:instruction_len] = [-100] * instruction_len\n",
    "\n",
    "    tokenized['labels'] = labels\n",
    "    return tokenized\n",
    "\n",
    "print(\"Токенизируем датасеты\")\n",
    "train_dataset = train_dataset.map(tokenize_function, remove_columns=['input', 'output'])\n",
    "val_dataset = val_dataset.map(tokenize_function, remove_columns=['input', 'output'])\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=MICRO_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=BATCH_SIZE // MICRO_BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    seed=42,\n",
    "    report_to=\"none\",\n",
    "    logging_strategy=\"steps\",\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_pin_memory=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "\n",
    "print(\"НАЧИНАЕМ ОБУЧЕНИЕ\")\n",
    "print(f\"Модель: {BASE_MODEL}\")\n",
    "print(f\"Эпохи: {NUM_EPOCHS}\")\n",
    "print(f\"Размер train: {len(train_dataset)}\")\n",
    "print(f\"Размер validation: {len(val_dataset)}\")\n",
    "\n",
    "trainer.train()\n",
    "print(\"Обучение завершено\")\n",
    "\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"Модель сохранена в {OUTPUT_DIR}\")\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "import shutil\n",
    "shutil.copytree(OUTPUT_DIR, f'/content/drive/MyDrive/{OUTPUT_DIR}', dirs_exist_ok=True)\n",
    "print(f\"Модель сохранена {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MSKPnZakq27m",
    "outputId": "81297f80-76f9-454b-f973-90920c6ebd0e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Запускаем валидацию\n",
      "Инициализация модели\n",
      "Загружаем модель\n",
      "Загружаем val_dataset.json\n",
      "Тестируем на 200 примерах\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Валидация: 100%|██████████| 200/200 [03:45<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "РЕЗУЛЬТАТЫ ВАЛИДАЦИИ:\n",
      "Общая точность: 25.00% (50/200)\n",
      "\n",
      "Точность по длине чисел:\n",
      "Длина 1: 100.00% (2/2)\n",
      "Длина 2: 85.71% (6/7)\n",
      "Длина 3: 72.73% (8/11)\n",
      "Длина 4: 60.00% (6/10)\n",
      "Длина 5: 40.00% (8/20)\n",
      "Длина 6: 26.92% (7/26)\n",
      "Длина 7: 13.64% (3/22)\n",
      "Длина 8: 14.71% (5/34)\n",
      "Длина 9: 4.00% (1/25)\n",
      "Длина 10: 9.30% (4/43)\n",
      "ТЕСТ НА ПРОСТЫХ ПРИМЕРАХ:\n",
      "Загружаем модель\n",
      "12 + 34 = 46 True\n",
      "5 + 7 = 12 True\n",
      "100 + 200 = 300 True\n",
      "999 + 1 = 1000 True\n",
      "25 + 75 = 100 True\n",
      "8 + 15 = 23 True\n",
      "Точность на простых примерах: 100.0%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "class ArithmeticModel:\n",
    "    def __init__(self, model_path):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Загружаем модель\")\n",
    "\n",
    "        try:\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "                device_map=\"auto\",\n",
    "                torch_dtype=torch.float16,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "\n",
    "            self.model = PeftModel.from_pretrained(self.model, model_path)\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "            self.model.eval()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка при загрузке модели: {e}\")\n",
    "\n",
    "    def generate_prompt(self, input_text):\n",
    "        return f\"Calculate: {input_text}\\nAnswer:\"\n",
    "\n",
    "    def add_numbers(self, a, b):\n",
    "        try:\n",
    "            input_text = f\"{a} + {b}\"\n",
    "            prompt = self.generate_prompt(input_text)\n",
    "\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=20,\n",
    "                    temperature=0.1,\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "\n",
    "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "            if \"Answer:\" in response:\n",
    "                answer_part = response.split(\"Answer:\")[1].strip()\n",
    "                numbers = re.findall(r'\\d+', answer_part)\n",
    "                if numbers:\n",
    "                    return numbers[0]\n",
    "\n",
    "            return \"0\"\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка при генерации: {e}\")\n",
    "            return \"0\"\n",
    "\n",
    "def validate_model(\n",
    "    model_path=\"./arithmetic_model_deep\",\n",
    "    test_data_path=\"val_dataset.json\",\n",
    "    max_tests=200\n",
    "    ):\n",
    "    \"\"\"Валидация модели на тестовых данных\"\"\"\n",
    "\n",
    "    print(\"Инициализация модели\")\n",
    "    model = ArithmeticModel(model_path)\n",
    "\n",
    "    print(f\"Загружаем {test_data_path}\")\n",
    "    with open(test_data_path, 'r') as f:\n",
    "        test_data = json.load(f)\n",
    "\n",
    "    test_subset = [ex for ex in test_data if ex['num_length'] <= 10][:max_tests]\n",
    "    print(f\"Тестируем на {len(test_subset)} примерах\")\n",
    "\n",
    "    correct = 0\n",
    "    results_by_length = {}\n",
    "\n",
    "    for example in tqdm(test_subset, desc=\"Валидация\"):\n",
    "        input_text = example['input']\n",
    "        expected = example['answer']\n",
    "\n",
    "        numbers = re.findall(r'\\d+', input_text)\n",
    "        if len(numbers) >= 2:\n",
    "            a, b = numbers[0], numbers[1]\n",
    "            predicted = model.add_numbers(a, b)\n",
    "\n",
    "            num_length = max(len(a), len(b))\n",
    "            if num_length not in results_by_length:\n",
    "                results_by_length[num_length] = {'correct': 0, 'total': 0}\n",
    "\n",
    "            results_by_length[num_length]['total'] += 1\n",
    "            if predicted == expected:\n",
    "                correct += 1\n",
    "                results_by_length[num_length]['correct'] += 1\n",
    "\n",
    "    accuracy = correct / len(test_subset) * 100\n",
    "\n",
    "    print(f\"РЕЗУЛЬТАТЫ ВАЛИДАЦИИ:\")\n",
    "    print(f\"Общая точность: {accuracy:.2f}% ({correct}/{len(test_subset)})\")\n",
    "\n",
    "    print(f\"\\nТочность по длине чисел:\")\n",
    "    for length in sorted(results_by_length.keys()):\n",
    "        stats = results_by_length[length]\n",
    "        acc = stats['correct'] / stats['total'] * 100\n",
    "        print(f\"Длина {length}: {acc:.2f}% ({stats['correct']}/{stats['total']})\")\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "print(\"Запускаем валидацию\")\n",
    "accuracy = validate_model(\"./arithmetic_model_deep\", \"val_dataset.json\")\n",
    "\n",
    "def quick_test():\n",
    "    print(\"ТЕСТ НА ПРОСТЫХ ПРИМЕРАХ:\")\n",
    "\n",
    "    model = ArithmeticModel(\"./arithmetic_model_deep\")\n",
    "\n",
    "    test_cases = [\n",
    "        (\"12\", \"34\"),\n",
    "        (\"5\", \"7\"),\n",
    "        (\"100\", \"200\"),\n",
    "        (\"999\", \"1\"),\n",
    "        (\"25\", \"75\"),\n",
    "        (\"8\", \"15\"),\n",
    "    ]\n",
    "\n",
    "    correct = 0\n",
    "    for a, b in test_cases:\n",
    "        predicted = model.add_numbers(a, b)\n",
    "        expected = str(int(a) + int(b))\n",
    "        status = \"True\" if predicted == expected else f\"False (ожидалось {expected})\"\n",
    "        if predicted == expected:\n",
    "            correct += 1\n",
    "        print(f\"{a} + {b} = {predicted} {status}\")\n",
    "\n",
    "    print(f\"Точность на простых примерах: {correct/len(test_cases)*100:.1f}%\")\n",
    "\n",
    "quick_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Za00DFYMM0q3"
   },
   "source": [
    "### Описание вашего решения\n",
    "\n",
    "[TODO]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K4gq2zr5xSYY"
   },
   "source": [
    "### 1. Генерация датасета для обучения\n",
    "- Основная идея: не генерить просто набор случайных примеров, а обеспечить многоуровневый подход с постепенным повышением сложности.\n",
    "- Простые числа - 3% датасета. Средние - 25%. Сложные - 72%. Общий размер получился около 68к примеров.\n",
    "- Split: 90% train / 10% validation\n",
    "\n",
    "### 2. Обучаем модель в первый раз\n",
    "- Ввиду ограниченности бесплатного Colab нужно было быстро понять в какую сторону двигаться, поэтому в данном обучении упор был сделан на экономию ресурса и скорость.\n",
    "- Берём не слишком тяжёлую `TinyLlama 1.1B`, 4-битную квантизацию, LoRA адаптация.\n",
    "- Ограничиваем обучение числами до 8 знаков, 5000 примеров для обучения и 1000 для валидации, простой промпт `Calculate: X + Y\\nAnswer: Z`.\n",
    "- Прочие настройки параметров для экономии ресурса.\n",
    "\n",
    "### 3. Валидация первой модели\n",
    "- Результат в целом адекватный, но есть большой простор для дальнейшей работы: общая точность 42%, хорошо справляется с числами по 1-4 знака, до 60% на 5-8 знаках, ожидаемо полностью проваливается на более 8 знаках.\n",
    "- `999 + 1 = 1099` вместо 1000. Модель понимает, что нужно прибавить, но ошибается в механизме переноса разрядов.\n",
    "\n",
    "### 4. Обучаем модель во второй раз\n",
    "- Решено было попробовать обучить с нуля с другим набором вводных: увеличим сложность и количество эпох, усложним промпт.\n",
    "- Попробуем другое распределение долей в датасете: сделаем упор на средние и сложные примеры. Специально добавим примеры с переносом разрядов.\n",
    "\n",
    "### 5. Валидация второй модели\n",
    "- Немного неожиданный и противоречивый результат.\n",
    "- Отличная точность на простых, есть прогресс на сложных примерах. Решена проблема с переносом.\n",
    "- Однако общая точность упала до 25%: резкий провал в среднем диапазоне.\n",
    "\n",
    "### 6. Анализ проблемы\n",
    "- Вероятно, модель могла переобучиться на специфических случаях типа переноса разрядов и потеряла обобщающую способность на обычных числах средней длины.\n",
    "- Увеличение сложности LoRA могло повлечь за собой нестабильность.\n",
    "- Плохо сбалансирован датасет.\n",
    "\n",
    "### 7. Возможные пути решения\n",
    "- В целом довольно типичная картина, когда оптимизация под конкретную задачу поломала общие возможности модели.\n",
    "- Надо хорошо подумать как сбалансировать датасет.\n",
    "- Поэкспериментировать с поэтапным дообучением адаптера с постепенным повышением сложности. Наверняка это поможет, но требует много ресурса.\n",
    "- Увеличить размер тестовой выборки, проводить и анализировать валидацию после каждой эпохи.\n",
    "\n",
    "### 8. Реализация хостинга\n",
    "- При помощи Gradio создаём хостинг с публичной ссылкой на простой интерфейс для взаимодействия с моделью."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-LC7YuaqM6n-"
   },
   "source": [
    "### *[1 балл] Дополнительное задание\n",
    "\n",
    "Реализовать хостинг вашей модели на gradio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 614
    },
    "id": "ZVNgeG6NuZ82",
    "outputId": "85b5c10e-96d3-42af-93fe-aa51a5fc7ce9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://bab201704b4b47b88c.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://bab201704b4b47b88c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "import re\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def create_interface():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"./arithmetic_model_deep\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"./arithmetic_model_deep\")\n",
    "    model.eval()\n",
    "\n",
    "    def calculate_simple(a, b):\n",
    "        try:\n",
    "            prompt = f\"Calculate: {a} + {b}\\nAnswer:\"\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=20,\n",
    "                    temperature=0.1,\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "\n",
    "            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            expected = str(int(a) + int(b))\n",
    "\n",
    "            answer = \"0\"\n",
    "            if \"Answer:\" in response:\n",
    "                answer_part = response.split(\"Answer:\")[1].strip()\n",
    "                numbers = re.findall(r'\\d+', answer_part)\n",
    "                if numbers:\n",
    "                    answer = numbers[0]\n",
    "\n",
    "            if answer == expected:\n",
    "                return f\"Правильно! {a} + {b} = {answer}\"\n",
    "            else:\n",
    "                return f\"Ошибка! Модель: {answer}, Правильно: {expected}\"\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"Ошибка: {str(e)}\"\n",
    "\n",
    "    with gr.Blocks(title=\"AI Калькулятор\") as demo:\n",
    "        gr.Markdown(\"AI Калькулятор\")\n",
    "\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                a_input = gr.Textbox(label=\"Первое число\")\n",
    "                b_input = gr.Textbox(label=\"Второе число\")\n",
    "                btn = gr.Button(\"Вычислить\", variant=\"primary\")\n",
    "\n",
    "            with gr.Column():\n",
    "                output = gr.Textbox(label=\"Результат\", interactive=False)\n",
    "\n",
    "        btn.click(calculate_simple, inputs=[a_input, b_input], outputs=output)\n",
    "\n",
    "    return demo\n",
    "\n",
    "demo = create_interface()\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](data:image/jpeg;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Lpj-ecDMJIg"
   },
   "source": [
    "### Выводы\n",
    "\n",
    "В этом задании вы научились решать арифметические действия с помощью языковой модели."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
