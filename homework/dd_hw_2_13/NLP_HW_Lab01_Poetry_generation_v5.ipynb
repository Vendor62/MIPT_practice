{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff-andcBIPPF"
      },
      "source": [
        "## Домашнее задание №9\n",
        "### Генерация поэзии с помощью нейронных сетей: шаг 1\n",
        "##### Автор: [Радослав Нейчев](https://www.linkedin.com/in/radoslav-neychev/), @neychev\n",
        "\n",
        "Ваша основная задача: научиться генерироват стихи с помощью простой рекуррентной нейронной сети (Vanilla RNN). В качестве корпуса текстов для обучения будет выступать роман в стихах \"Евгений Онегин\" Александра Сергеевича Пушкина."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "k-S2AEZRIPPI"
      },
      "outputs": [],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "import string\n",
        "import os\n",
        "from random import sample\n",
        "\n",
        "import numpy as np\n",
        "import torch, torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aL4IEXmmIPPJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda device is available\n"
          ]
        }
      ],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"{} device is available\".format(device))\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPenWOy01Ooa",
        "outputId": "a92e8e33-e009-4bd4-ac12-3b1b5e1cd3f2"
      },
      "source": [
        "#### 1. Загрузка данных."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4E1cntH2IPPK"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\"wget\" �� ���� ����७��� ��� ���譥�\n",
            "��������, �ᯮ��塞�� �ணࠬ��� ��� ������ 䠩���.\n"
          ]
        }
      ],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "!wget https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/onegin.txt\n",
        "\n",
        "with open('onegin.txt', 'r', encoding='utf-8') as iofile:\n",
        "    text = iofile.readlines()\n",
        "\n",
        "text = \"\".join([x.replace('\\t\\t', '').lower() for x in text])\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQYpmGfR_gJ8"
      },
      "source": [
        "#### 2. Построение словаря и предобработка текста\n",
        "В данном задании требуется построить языковую модель на уровне символов. Приведем весь текст к нижнему регистру и построим словарь из всех символов в доступном корпусе текстов. Также добавим токен `<sos>`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nVQR5pqWIPPL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seems fine!\n"
          ]
        }
      ],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "tokens = sorted(set(text.lower())) + [\"<sos>\"]\n",
        "num_tokens = len(tokens)\n",
        "\n",
        "assert num_tokens == 84, \"Check the tokenization process\"\n",
        "\n",
        "token_to_idx = {x: idx for idx, x in enumerate(tokens)}\n",
        "idx_to_token = {idx: x for idx, x in enumerate(tokens)}\n",
        "\n",
        "assert len(tokens) == len(token_to_idx), \"Mapping should be unique\"\n",
        "\n",
        "print(\"Seems fine!\")\n",
        "\n",
        "\n",
        "text_encoded = [token_to_idx[x] for x in text]\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgQZgS8TIPPL"
      },
      "source": [
        "__Ваша задача__: обучить классическую рекуррентную нейронную сеть (Vanilla RNN) предсказывать следующий символ на полученном корпусе текстов и сгенерировать последовательность длины 100 для фиксированной начальной фразы.\n",
        "\n",
        "Вы можете воспользоваться кодом с занятие №6 или же обратиться к следующим ссылкам:\n",
        "* Замечательная статья за авторством Andrej Karpathy об использовании RNN: [link](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
        "* Пример char-rnn от Andrej Karpathy: [github repo](https://github.com/karpathy/char-rnn)\n",
        "* Замечательный пример генерации поэзии Шекспира: [github repo](https://github.com/spro/practical-pytorch/blob/master/char-rnn-generation/char-rnn-generation.ipynb)\n",
        "\n",
        "Данное задание является достаточно творческим. Не страшно, если поначалу оно вызывает затруднения. Последняя ссылка в списке выше может быть особенно полезна в данном случае.\n",
        "\n",
        "Далее для вашего удобства реализована функция, которая генерирует случайный батч размера `batch_size` из строк длиной `seq_length`. Вы можете использовать его при обучении модели."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Ll36DOYKIPPM"
      },
      "outputs": [],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "batch_size = 256\n",
        "seq_length = 100\n",
        "start_column = np.zeros((batch_size, 1), dtype=int) + token_to_idx[\"<sos>\"]\n",
        "\n",
        "\n",
        "def generate_chunk():\n",
        "    global text_encoded, start_column, batch_size, seq_length\n",
        "\n",
        "    start_index = np.random.randint(0, len(text_encoded) - batch_size * seq_length - 1)\n",
        "    data = np.array(\n",
        "        text_encoded[start_index : start_index + batch_size * seq_length]\n",
        "    ).reshape((batch_size, -1))\n",
        "    yield np.hstack((start_column, data))\n",
        "\n",
        "\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGADcoBoIPPN"
      },
      "source": [
        "Пример батча:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0THAzyV-IPPN",
        "outputId": "1384d4da-2107-4a97-d9eb-c3bfc95439c2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[83, 59, 57, ...,  0, 64,  1],\n",
              "       [83, 56, 45, ...,  7,  0, 47],\n",
              "       [83, 62, 50, ..., 51, 49, 45],\n",
              "       ...,\n",
              "       [83, 47, 59, ..., 55, 45, 60],\n",
              "       [83, 53, 63, ..., 26, 26,  0],\n",
              "       [83,  0, 55, ...,  1, 58, 50]])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "next(generate_chunk())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQlm-A5VIPPO"
      },
      "source": [
        "Далее вам предстоит написать код для обучения модели и генерации текста."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "V8MX1BZoIPPO"
      },
      "outputs": [],
      "source": [
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super(CharRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        out, hidden = self.rnn(x, hidden)\n",
        "        out = self.fc(out)\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(1, batch_size, self.hidden_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 4.4532\n",
            "Epoch 2, Loss: 4.1200\n",
            "Epoch 3, Loss: 3.7261\n",
            "Epoch 4, Loss: 3.3555\n",
            "Epoch 5, Loss: 3.1758\n",
            "Epoch 6, Loss: 3.0721\n",
            "Epoch 7, Loss: 3.0363\n",
            "Epoch 8, Loss: 2.9615\n",
            "Epoch 9, Loss: 2.9054\n",
            "Epoch 10, Loss: 2.8633\n",
            "Epoch 11, Loss: 2.8399\n",
            "Epoch 12, Loss: 2.7836\n",
            "Epoch 13, Loss: 2.8007\n",
            "Epoch 14, Loss: 2.7586\n",
            "Epoch 15, Loss: 2.7203\n",
            "Epoch 16, Loss: 2.6928\n",
            "Epoch 17, Loss: 2.6647\n",
            "Epoch 18, Loss: 2.6932\n",
            "Epoch 19, Loss: 2.6331\n",
            "Epoch 20, Loss: 2.6124\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model = CharRNN(vocab_size=num_tokens, hidden_size=128).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "n_epochs = 20\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    hidden = model.init_hidden(batch_size).to(device)\n",
        "\n",
        "    for batch in generate_chunk():\n",
        "        x = torch.tensor(batch[:, :-1], dtype=torch.long).to(device)\n",
        "        y = torch.tensor(batch[:, 1:], dtype=torch.long).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output, hidden = model(x, hidden.detach())\n",
        "        loss = loss_fn(output.reshape(-1, num_tokens), y.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxJej2JSIPPP"
      },
      "source": [
        "Шаблон функции `generate_sample` также доступен ниже. Вы можете как дозаполнить его, так и написать свою собственную функцию с нуля. Не забывайте, что все примеры в обучающей выборке начинались с токена `<sos>`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "weFPtZ6OIPPP"
      },
      "outputs": [],
      "source": [
        "def generate_sample(\n",
        "    char_rnn, seed_phrase=None, max_length=500, temperature=1.0, device=device\n",
        "):\n",
        "    \"\"\"\n",
        "    The function generates text given a phrase of length at least SEQ_LENGTH.\n",
        "    :param seed_phrase: prefix characters. The RNN is asked to continue the phrase\n",
        "    :param max_length: maximum output length, including seed_phrase\n",
        "    :param temperature: coefficient for sampling.  higher temperature produces more chaotic outputs,\n",
        "                        smaller temperature converges to the single most likely output\n",
        "    \"\"\"\n",
        "    if seed_phrase is not None:\n",
        "        x_sequence = [token_to_idx[\"<sos>\"]] + [\n",
        "            token_to_idx[token] for token in seed_phrase\n",
        "        ]\n",
        "    else:\n",
        "        x_sequence = [token_to_idx[\"<sos>\"]]\n",
        "\n",
        "    x_sequence = torch.tensor([x_sequence], dtype=torch.int64).to(device)\n",
        "\n",
        "    char_rnn.eval()\n",
        "    hidden = char_rnn.init_hidden(1).to(device)\n",
        "    \n",
        "    for i in range(x_sequence.shape[1] - 1):\n",
        "        _, hidden = char_rnn(x_sequence[:, i:i+1], hidden)\n",
        "\n",
        "    input_char = x_sequence[:, -1:]\n",
        "\n",
        "    while x_sequence.shape[1] < max_length:\n",
        "        out, hidden = char_rnn(input_char, hidden)\n",
        "        logits = out[:, -1, :] / temperature\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        next_char = torch.multinomial(probs, 1)\n",
        "        x_sequence = torch.cat([x_sequence, next_char], dim=1)\n",
        "        input_char = next_char\n",
        "\n",
        "    return \"\".join([tokens[ix] for ix in x_sequence.cpu().data.numpy()[0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiHkwpbXIPPP"
      },
      "source": [
        "Пример текста сгенерированного обученной моделью доступен ниже. Не страшно, что в тексте много несуществующих слов. Используемая модель очень проста: это простая классическая RNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "735k_eRMIPPP",
        "outputId": "be032842-ddb3-4f7f-f3aa-3f1ff1a7a06b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<sos> мой дядя самых честных правила\n",
            "жова,\n",
            "москомед\n",
            "ранесц опой пененный ваихтаянитвавсожнод дреец, потьной каз ой гой илевеет падан чене\n",
            "ковь,\n",
            "мог глапыем пут ватокой босрака прем едиума тет, сти гровувюнойи\n",
            "о стредгой\n",
            "ков стале  свеголопил, бовоце понь:\n",
            "о пождавы вальен межсь свелим дузннечный пхогукой но поготь п м ол,\n",
            "\n",
            "\n",
            "\n",
            "у вогоскени мний стогомиковлонпожиль мрестой вове я отьви строви постовестугол вжиньт нои жтал празог транне вунир\n",
            " медо подны,\n",
            "ивкони м проз теобывех тевеко на нат мед, псыре у\n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "    generate_sample(\n",
        "        model, \" мой дядя самых честных правил\", max_length=500, temperature=0.8\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnktcBEuIPPP"
      },
      "source": [
        "### Сдача задания\n",
        "Сгенерируйте десять последовательностей длиной 500, используя строку ' мой дядя самых честных правил'. Температуру для генерации выберите самостоятельно на основании визуального качества генериуремого текста. Не забудьте удалить все технические токены в случае их наличия.\n",
        "\n",
        "Сгенерированную последовательность сохрание в переменную `generated_phrase` и сдайте сгенерированный ниже файл в контест."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "9oGZPxQjIPPQ"
      },
      "outputs": [],
      "source": [
        "seed_phrase = \" мой дядя самых честных правил\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDJDolY6IPPQ"
      },
      "outputs": [],
      "source": [
        "generated_phrases = [\n",
        "    generate_sample(\n",
        "        model,\n",
        "        seed_phrase=seed_phrase,\n",
        "        max_length=501,\n",
        "        temperature=0.8\n",
        "    ).replace('<sos>', '')\n",
        "    for _ in range(10)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "tiRLf_BvIPPQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File saved to `submission_dict_hw09.npy`\n"
          ]
        }
      ],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "\n",
        "if \"generated_phrases\" not in locals():\n",
        "    raise ValueError(\"Please, save generated phrases to `generated_phrases` variable\")\n",
        "\n",
        "for phrase in generated_phrases:\n",
        "\n",
        "    if not isinstance(phrase, str):\n",
        "        raise ValueError(\"The generated phrase should be a string\")\n",
        "\n",
        "    if len(phrase) != 500:\n",
        "        raise ValueError(\"The `generated_phrase` length should be equal to 500\")\n",
        "\n",
        "    assert all(\n",
        "        [x in set(tokens) for x in set(list(phrase))]\n",
        "    ), \"Unknown tokens detected, check your submission!\"\n",
        "\n",
        "\n",
        "submission_dict = {\"token_to_idx\": token_to_idx, \"generated_phrases\": generated_phrases}\n",
        "\n",
        "np.save(\"submission_dict_hw09.npy\", submission_dict, allow_pickle=True)\n",
        "print(\"File saved to `submission_dict_hw09.npy`\")\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxVjJjHwIPPQ"
      },
      "source": [
        "На этом задание завершено. Поздравляем!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "name": "NLP HW Lab01_Poetry_generation.v5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
