{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "termcolor not installed, skipping dependency\n",
      "No pygame installed, ignoring import\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import logging\n",
    "import pandas as pd\n",
    "from kaggle_environments import make, evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 0 - rock\n",
    "- 1 - paper\n",
    "- 2 - scissors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='game_logs.txt', level=logging.INFO, format='%(message)s', filemode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Последние три агента пользуются функцией get_score для выяснения результата хода."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(left_move, right_move):\n",
    "    \"\"\"\n",
    "    Определяет результат взаимодействия между двумя жестами.\n",
    "\n",
    "    Функция принимает два жеста и вычисляет, какой из них выигрывает.\n",
    "    Если оба жеста равны, возвращает 0 (ничья). Если первый жест (left_move)\n",
    "    выигрывает, возвращает 1. Если второй жест (right_move) выигрывает,\n",
    "    возвращает -1.\n",
    "\n",
    "    Args:\n",
    "        left_move (int): Жест первого игрока, может быть 0 (камень), 1 (бумага) или 2 (ножницы).\n",
    "        right_move (int): Жест второго игрока, может быть 0 (камень), 1 (бумага) или 2 (ножницы).\n",
    "\n",
    "    Returns:\n",
    "        int: 1, если первый жест выигрывает; -1, если второй жест выигрывает; 0, если ничья.\n",
    "    \"\"\"\n",
    "    \n",
    "    delta = (\n",
    "        right_move - left_move\n",
    "        \n",
    "        if (left_move + right_move) % 2 == 0\n",
    "        else left_move - right_move\n",
    "    )\n",
    "    return 0 if delta == 0 else math.copysign(1, delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первые три агента будут всегда выбрасывать либо 0, либо 1, либо 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constant_rock(observation, configuration):\n",
    "    \"\"\"\n",
    "    Агент, который всегда выбирает жест \"камень\".\n",
    "\n",
    "    Этот агент не меняет свой выбор и постоянно выбрасывает камень \n",
    "    в каждом раунде. Это делает его предсказуемым для противника, \n",
    "    так как он не адаптируется к действиям соперника.\n",
    "\n",
    "    Args:\n",
    "        observation (object): Наблюдение о состоянии текущего раунда, включая:\n",
    "                              - observation.step (int): текущий номер шага.\n",
    "                              - observation.reward (int): текущая награда.\n",
    "        configuration (object): Конфигурация игры, включая:\n",
    "                                - configuration.signs (int): количество возможных жестов.\n",
    "\n",
    "    Returns:\n",
    "        int: Константный жест \"камень\", представленный целым числом 0.\n",
    "    \"\"\"\n",
    "    \n",
    "    move = 0 \n",
    "    \n",
    "    logging.info(\n",
    "        f\"constant_rock: \"\n",
    "        f\"step={observation.step}, \"\n",
    "        f\"move={move}, \"\n",
    "        f\"reward={observation.reward}\"\n",
    "    )    \n",
    "    return move\n",
    "\n",
    "def constant_paper(observation, configuration):\n",
    "    \"\"\"\n",
    "    Агент, который всегда выбирает жест \"бумага\".\n",
    "\n",
    "    Этот агент не меняет свой выбор и постоянно выбрасывает бумагу \n",
    "    в каждом раунде. Это делает его предсказуемым для противника, \n",
    "    так как он не адаптируется к действиям соперника.\n",
    "\n",
    "    Args:\n",
    "        observation (object): Наблюдение о состоянии текущего раунда, включая:\n",
    "                              - observation.step (int): текущий номер шага.\n",
    "                              - observation.reward (int): текущая награда.\n",
    "        configuration (object): Конфигурация игры, включая:\n",
    "                                - configuration.signs (int): количество возможных жестов.\n",
    "\n",
    "    Returns:\n",
    "        int: Константный жест \"бумага\", представленный целым числом 1.\n",
    "    \"\"\"\n",
    "    \n",
    "    move = 1 \n",
    "    \n",
    "    logging.info(\n",
    "        f\"constant_paper: \"\n",
    "        f\"step={observation.step}, \"\n",
    "        f\"move={move}, \"\n",
    "        f\"reward={observation.reward}\"\n",
    "    )    \n",
    "    return move\n",
    "\n",
    "def constant_scissors(observation, configuration):\n",
    "    \"\"\"\n",
    "    Агент, который всегда выбирает жест \"ножницы\".\n",
    "\n",
    "    Этот агент не меняет свой выбор и постоянно выбрасывает ножницы \n",
    "    в каждом раунде. Это делает его предсказуемым для противника, \n",
    "    так как он не адаптируется к действиям соперника.\n",
    "\n",
    "    Args:\n",
    "        observation (object): Наблюдение о состоянии текущего раунда, включая:\n",
    "                              - observation.step (int): текущий номер шага.\n",
    "                              - observation.reward (int): текущая награда.\n",
    "        configuration (object): Конфигурация игры, включая:\n",
    "                                - configuration.signs (int): количество возможных жестов.\n",
    "\n",
    "    Returns:\n",
    "        int: Константный жест \"ножницы\", представленный целым числом 2.\n",
    "    \"\"\"\n",
    "    \n",
    "    move = 2 \n",
    "    \n",
    "    logging.info(\n",
    "        f\"constant_scissors: \"\n",
    "        f\"step={observation.step}, \"\n",
    "        f\"move={move}, \"\n",
    "        f\"reward={observation.reward}\"\n",
    "    )    \n",
    "    return move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее будет агент, который всегда выбрасывает рандомные значения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sign(observation, configuration):\n",
    "    \"\"\"\n",
    "    Агент, который всегда выбирает случайный жест.\n",
    "\n",
    "    Этот агент не следует никаким стратегиям или шаблонам, а просто\n",
    "    выбирает жест случайным образом из доступных вариантов. Это может\n",
    "    сделать его предсказуемым для противника, но также позволяет\n",
    "    избежать привязки к определенной стратегии.\n",
    "\n",
    "    Args:\n",
    "        observation (object): Наблюдение о состоянии текущего раунда, включая:\n",
    "                              - observation.step (int): текущий номер шага.\n",
    "                              - observation.reward (int): текущая награда.\n",
    "        configuration (object): Конфигурация игры, включая:\n",
    "                                - configuration.signs (int): количество возможных жестов.\n",
    "\n",
    "    Returns:\n",
    "        int: Случайный жест агента, выбранный из диапазона [0, configuration.signs - 1].\n",
    "    \"\"\"\n",
    "    \n",
    "    move = random.randrange(0, configuration.signs)\n",
    "    \n",
    "    logging.info(\n",
    "        f\"random_sign: \"\n",
    "        f\"step={observation.step}, \"\n",
    "        f\"move={move}, \"\n",
    "        f\"reward={observation.reward}\"\n",
    "    )    \n",
    "    return move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем два агента с ротацией: sequence всегда чередует [0, 1, 2], а rotation каждые три хода будет случайным образом генерировать свою последовательность."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequence():\n",
    "    \"\"\"\n",
    "    Создаёт агент, который делает ходы в строгой последовательности.\n",
    "\n",
    "    Агент выполняет ходы в порядке: камень, ножницы, бумага. После того как\n",
    "    все жесты будут использованы, последовательность начинается заново.\n",
    "\n",
    "    Returns:\n",
    "        function: Функция `sequence`, которая принимает `observation` и\n",
    "                  `configuration` и возвращает целое число (ход агента)\n",
    "                  от 0 до `configuration.signs - 1`.\n",
    "    \"\"\"\n",
    "    \n",
    "    sign = 0\n",
    "    order = [0, 1, 2] \n",
    "\n",
    "    def sequence(observation, configuration):\n",
    "        \"\"\"\n",
    "        Выбирает текущий жест согласно установленной последовательности.\n",
    "\n",
    "        Args:\n",
    "            observation (object): Наблюдение о состоянии текущего раунда, включая:\n",
    "                                  - observation.step (int): текущий номер шага.\n",
    "                                  - observation.reward (int): текущая награда.\n",
    "            configuration (object): Конфигурация игры, включая:\n",
    "                                    - configuration.signs (int): количество возможных жестов.\n",
    "\n",
    "        Returns:\n",
    "            int: Жест агента, который следует текущей последовательности.\n",
    "        \"\"\"\n",
    "        \n",
    "        nonlocal sign\n",
    "        \n",
    "        move = order[sign] \n",
    "        sign += 1\n",
    "            \n",
    "        if sign >= len(order):\n",
    "            sign = 0 \n",
    "            \n",
    "        logging.info(\n",
    "            f\"sequence: \"\n",
    "            f\"step={observation.step}, \"\n",
    "            f\"move={move}, \"\n",
    "            f\"reward={observation.reward}\"\n",
    "        )    \n",
    "        return move  \n",
    "    return sequence\n",
    "\n",
    "def create_rotation():\n",
    "    \"\"\"\n",
    "    Создаёт случайную ротацию ходов и следует ей.\n",
    "\n",
    "    Агент генерирует случайный порядок ходов (камень, ножницы, бумага) и\n",
    "    выполняет их по очереди. После завершения ротации порядок обновляется\n",
    "    и начинается заново.\n",
    "\n",
    "    Returns:\n",
    "        function: Функция `rotation`, которая принимает `observation` и\n",
    "                  `configuration` и возвращает целое число (ход агента)\n",
    "                  от 0 до `configuration.signs - 1`.\n",
    "    \"\"\"\n",
    "    \n",
    "    sign = 0\n",
    "    order = [0, 1, 2]\n",
    "    random.shuffle(order) \n",
    "    \n",
    "    def rotation(observation, configuration):\n",
    "        \"\"\"\n",
    "        Выбирает текущий жест согласно ротации.\n",
    "\n",
    "        Args:\n",
    "            observation (object): Наблюдение о состоянии текущего раунда, включая:\n",
    "                                  - observation.step (int): текущий номер шага.\n",
    "                                  - observation.reward (int): текущая награда.\n",
    "            configuration (object): Конфигурация игры, включая:\n",
    "                                    - configuration.signs (int): количество возможных жестов.\n",
    "\n",
    "        Returns:\n",
    "            int: Жест агента, который следует текущему порядку ротации.\n",
    "        \"\"\"\n",
    "        \n",
    "        nonlocal sign \n",
    "        move = order[sign]  \n",
    "        sign += 1\n",
    "        \n",
    "        if sign >= len(order):\n",
    "            random.shuffle(order)  \n",
    "            sign = 0 \n",
    "\n",
    "        logging.info(\n",
    "            f\"rotation: \"\n",
    "            f\"step={observation.step}, \"\n",
    "            f\"move={move}, \"\n",
    "            f\"reward={observation.reward}\"\n",
    "        )        \n",
    "        return move\n",
    "    return rotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее два агента, которые действуют исходя из последнего хода противника: copy_opponent копирует его, respond_to_opponent выбирает противодействующий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_opponent(observation, configuration):\n",
    "    \"\"\"\n",
    "    Повторяет последний жест противника.\n",
    "\n",
    "    Агент на первом шаге делает случайный выбор жеста. На всех последующих\n",
    "    шагах он копирует последний ход противника.\n",
    "\n",
    "    Returns:\n",
    "        int: Жест, который будет повторять последний ход противника,\n",
    "              возвращаемое значение от 0 до `configuration.signs - 1`.\n",
    "    \"\"\"\n",
    "    \n",
    "    if observation.step > 0:\n",
    "        move = observation.lastOpponentAction  \n",
    "    else:\n",
    "        move = random.randrange(0, configuration.signs)  \n",
    "\n",
    "    logging.info(\n",
    "        f\"copy_opponent: \"\n",
    "        f\"step={observation.step}, \"\n",
    "        f\"move={move}, \"\n",
    "        f\"reward={observation.reward}\"\n",
    "    )    \n",
    "    return move\n",
    "\n",
    "def respond_to_opponent(observation, configuration):\n",
    "    \"\"\"\n",
    "    Выбирает жест, который побеждает предыдущий жест противника.\n",
    "\n",
    "    Агент на первом шаге делает случайный выбор жеста. На следующих шагах он\n",
    "    анализирует последний жест противника и выбирает жест, который его контрит.\n",
    "    \n",
    "    Returns:\n",
    "        int: Выбранный жест агента, который будет контрить последний ход\n",
    "              противника, возвращаемое значение от 0 до `configuration.signs - 1`.\n",
    "    \"\"\"\n",
    "    \n",
    "    if observation.step == 0:\n",
    "        move = random.randrange(0, configuration.signs)  \n",
    "    else:\n",
    "        opponent_move = observation.lastOpponentAction  \n",
    "        \n",
    "        if opponent_move == 0:  \n",
    "            move = 1  \n",
    "        elif opponent_move == 1:  \n",
    "            move = 2  \n",
    "        elif opponent_move == 2:  \n",
    "            move = 0  \n",
    "\n",
    "        logging.info(\n",
    "            f\"respond_to_opponent: \"\n",
    "            f\"step={observation.step}, \"\n",
    "            f\"move={move}, \"\n",
    "            f\"reward={observation.reward}\"\n",
    "        )    \n",
    "    return move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее агент, который накапливает информацию о количестве ходов, которые делает противник, и выбирает контрящий ходу с наибольшим счетчиком."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_common_selection():\n",
    "    \"\"\"\n",
    "    Создает агента, который противодействует наиболее частому ходу противника.\n",
    "\n",
    "    Этот агент отслеживает количество раз, когда противник выбрал каждый из\n",
    "    возможных жестов (камень, ножницы, бумага), и выбирает жест, который\n",
    "    контрит наиболее частый ход противника. Если противник часто повторяет\n",
    "    один и тот же жест, агент будет адаптироваться к этому шаблону.\n",
    "\n",
    "    Returns:\n",
    "        function: Функция `common_selection`, которая принимает `observation` и\n",
    "                  `configuration` и возвращает целое число (ход агента) от 0 до\n",
    "                  `configuration.signs - 1`.\n",
    "    \"\"\"\n",
    "    \n",
    "    stats = {0: 0, 1: 0, 2: 0}\n",
    "    \n",
    "    def common_selection(observation, configuration):\n",
    "        \"\"\"\n",
    "        Определяет жест агента на основе статистики последних ходов противника.\n",
    "\n",
    "        Args:\n",
    "            observation (object): Наблюдение о состоянии текущего раунда, включая:\n",
    "                                  - observation.step (int): текущий номер шага.\n",
    "                                  - observation.lastOpponentAction (int): жест, выбранный противником.\n",
    "            configuration (object): Конфигурация игры, включая:\n",
    "                                    - configuration.signs (int): количество возможных жестов.\n",
    "\n",
    "        Returns:\n",
    "            int: Выбранный жест агента, который будет контрить наиболее частый\n",
    "                 жест противника.\n",
    "        \"\"\"\n",
    "        \n",
    "        nonlocal stats\n",
    "        \n",
    "        if observation.step > 0:\n",
    "            last_move = observation.lastOpponentAction\n",
    "            stats[last_move] += 1  \n",
    "        else:\n",
    "            return random.randrange(0, configuration.signs)  \n",
    "\n",
    "        most_common_move = max(stats, key=stats.get)\n",
    "\n",
    "        if most_common_move == 0:\n",
    "            move = 1 \n",
    "        elif most_common_move == 1:\n",
    "            move = 2  \n",
    "        else:\n",
    "            move = 0 \n",
    "\n",
    "        logging.info(\n",
    "            f\"common_selection: \"\n",
    "            f\"step={observation.step}, \"\n",
    "            f\"move={move}, \"\n",
    "            f\"reward={observation.reward}, \"\n",
    "            f\"stats={stats}\"\n",
    "        )\n",
    "        return move\n",
    "    return common_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Агент, который проверяет своё текущее состояние reward. Если оно слишком низкое, то он играет по тактике respond_to_opponent, если среднее, то по тактике random_sign, если высокое, то пытается защищаться, создавая ротацию, в которой чаще будут присутствовать только ножницы и иногда разбавляя её камнем или бумагой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reward_check():\n",
    "    \"\"\"\n",
    "    Создает агента, который принимает решения на основе полученной награды и\n",
    "    последнего хода противника.\n",
    "\n",
    "    Этот агент использует три различных стратегии в зависимости от награды:\n",
    "    - Если награда меньше или равна 25, агент выбирает ход, который контрит\n",
    "      последний ход противника.\n",
    "    - Если награда больше 25 и меньше или равна 75, агент выбирает случайный\n",
    "      ход.\n",
    "    - Если награда больше 75, агент использует заранее определенный порядок\n",
    "      ходов, который повторяется после использования трех ходов.\n",
    "\n",
    "    Returns:\n",
    "        function: Функция `reward_check`, которая принимает `observation` и\n",
    "                  `configuration` и возвращает целое число (ход агента) от 0 до\n",
    "                  `configuration.signs - 1`.\n",
    "    \"\"\"\n",
    "    \n",
    "    sign = 0\n",
    "    order = [2, 2, 2, 0, 1]  \n",
    "    order_def = random.sample(order, 3)\n",
    "    \n",
    "    def reward_check(observation, configuration):\n",
    "        \"\"\"\n",
    "        Определяет жест агента на основе текущего шага и награды.\n",
    "\n",
    "        Args:\n",
    "            observation (object): Наблюдение о состоянии текущего раунда, включая:\n",
    "                                  - observation.step (int): текущий номер шага.\n",
    "                                  - observation.lastOpponentAction (int): жест, выбранный противником.\n",
    "                                  - observation.reward (int): награда, полученная за предыдущий ход.\n",
    "            configuration (object): Конфигурация игры, включая:\n",
    "                                    - configuration.signs (int): количество возможных жестов.\n",
    "\n",
    "        Returns:\n",
    "            int: Выбранный жест агента, который будет основан на логике награды и\n",
    "                 ходах противника.\n",
    "        \"\"\"\n",
    "        \n",
    "        nonlocal sign, order_def\n",
    "    \n",
    "        if observation.step == 0:\n",
    "            move = random.randrange(0, configuration.signs)\n",
    "        else:\n",
    "            if observation.reward <= 25:\n",
    "                opponent_move = observation.lastOpponentAction\n",
    "            \n",
    "                if opponent_move == 0:  \n",
    "                    move = 1 \n",
    "                elif opponent_move == 1:  \n",
    "                    move = 2 \n",
    "                elif opponent_move == 2:  \n",
    "                    move = 0 \n",
    "                         \n",
    "            elif 25 < observation.reward <= 75:\n",
    "                move = random.randrange(0, configuration.signs)  \n",
    "            else:\n",
    "                move = order_def[sign]\n",
    "                sign += 1\n",
    "                \n",
    "                if sign >= len(order_def):  \n",
    "                    order_def = random.sample(order, 3)  \n",
    "                    sign = 0 \n",
    "                    \n",
    "        logging.info(\n",
    "            f\"reward_check: \"\n",
    "            f\"step={observation.step}, \"\n",
    "            f\"move={move}, \"\n",
    "            f\"reward={observation.reward} \"\n",
    "        )            \n",
    "        return move\n",
    "    return reward_check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Агент, который будет избегать повторений одного жеста дважды подряд."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anti_replay():\n",
    "    \"\"\"\n",
    "    Создает агента, который избегает повторения одного и того же хода дважды подряд.\n",
    "\n",
    "    Этот агент выбирает случайный ход на первом шаге, а затем на каждом последующем\n",
    "    выбирает случайный ход, отличный от предыдущего. Это помогает избежать предсказуемости\n",
    "    для оппонента.\n",
    "\n",
    "    Returns:\n",
    "        function: Функция `anti_replay`, которая принимает `observation` и `configuration`\n",
    "                  и возвращает целое число (ход агента) от 0 до `configuration.signs - 1`.\n",
    "    \"\"\"\n",
    "    \n",
    "    last_move = None\n",
    "    order = [0, 1, 2]\n",
    "\n",
    "    def anti_replay(observation, configuration):\n",
    "        \"\"\"\n",
    "        Выбирает жест, который отличается от последнего выполненного жеста.\n",
    "\n",
    "        Args:\n",
    "            observation (object): Наблюдение о состоянии текущего раунда, включая:\n",
    "                                  - observation.step (int): текущий номер шага.\n",
    "                                  - observation.lastOpponentAction (int): жест, выбранный противником.\n",
    "            configuration (object): Конфигурация игры, включая:\n",
    "                                    - configuration.signs (int): количество возможных жестов.\n",
    "\n",
    "        Returns:\n",
    "            int: Выбранный жест агента, который будет отличаться от предыдущего.\n",
    "        \"\"\"\n",
    "        \n",
    "        nonlocal last_move, order\n",
    "        \n",
    "        if observation.step == 0:\n",
    "            move = random.choice(order)\n",
    "        else:\n",
    "            new_order = [i for i in order if i != last_move]\n",
    "            move = random.choice(new_order)  \n",
    "            \n",
    "        last_move = move \n",
    "        \n",
    "        logging.info(\n",
    "            f\"anti_replay: \"\n",
    "            f\"step={observation.step}, \"\n",
    "            f\"move={move}, \"\n",
    "            f\"reward={observation.reward} \"\n",
    "        )     \n",
    "        return move  \n",
    "    return anti_replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Агент, который попробует предсказать ход противника на основе вероятностей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_predictive_agent():\n",
    "    \"\"\"\n",
    "    Создает агента-предсказателя, который выбирает ход на основе вероятности действий противника.\n",
    "\n",
    "    Агент отслеживает количество каждого хода противника и вычисляет вероятность каждого\n",
    "    из них. На основе этих вероятностей агент выбирает оптимальный ответ, добавляя\n",
    "    элемент случайности, чтобы оставаться менее предсказуемым.\n",
    "\n",
    "    Returns:\n",
    "        function: Функция `predictive_agent`, которая принимает `observation` и `configuration`\n",
    "                  и возвращает целое число (ход агента) от 0 до `configuration.signs - 1`.\n",
    "    \"\"\"\n",
    "    action_counts = {0: 0, 1: 0, 2: 0}  \n",
    "    total_moves = 0  \n",
    "\n",
    "    def predictive_agent(observation, configuration):\n",
    "        \"\"\"\n",
    "        Выбирает ход на основе вероятностей предыдущих действий противника.\n",
    "\n",
    "        Args:\n",
    "            observation (object): Наблюдение о состоянии текущего раунда, включая:\n",
    "                                  - observation.step (int): текущий номер шага.\n",
    "                                  - observation.lastOpponentAction (int): жест, выбранный противником.\n",
    "            configuration (object): Конфигурация игры, включая:\n",
    "                                    - configuration.signs (int): количество возможных жестов.\n",
    "\n",
    "        Returns:\n",
    "            int: Выбранный жест агента, который будет оптимален против наиболее вероятного\n",
    "                 хода противника.\n",
    "        \"\"\"\n",
    "        nonlocal action_counts, total_moves\n",
    "        \n",
    "        if observation.step == 0:\n",
    "            return random.randrange(0, configuration.signs)  \n",
    "        \n",
    "        opponent_move = observation.lastOpponentAction\n",
    "        action_counts[opponent_move] += 1\n",
    "        total_moves += 1\n",
    "        probabilities = {move: count / total_moves for move, count in action_counts.items()}\n",
    "        best_move = (opponent_move + 1) % configuration.signs  \n",
    "\n",
    "        if random.random() < 0.8:  \n",
    "            move = best_move\n",
    "        else:\n",
    "            move = random.randrange(0, configuration.signs) \n",
    "        \n",
    "        logging.info(\n",
    "            f\"predictive_agent: \"\n",
    "            f\"step={observation.step}, \"\n",
    "            f\"move={move}, \"\n",
    "            f\"reward={observation.reward}, \"\n",
    "            f\"probabilities={probabilities}\"\n",
    "        )             \n",
    "        return move\n",
    "    return predictive_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- reactionary: стремится контрить последний ход противника, логика такая же как и у respond_to_opponent, но немного иначе реализованная\n",
    "- counter_reactionary: также контрит последний ход, но если его ход оказался выигрышным, то он повторит свой выигрышный ход\n",
    "- statistical: продвинутая версия common_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reactionary():\n",
    "    \"\"\"\n",
    "    Создает агента, который реагирует на ход противника, выбирая жест, который контрит его предыдущий ход.\n",
    "    \n",
    "    Агент делает случайный выбор на первом шаге, а затем на каждом последующем шаге\n",
    "    выбирает жест, который побеждает последний ход противника, если предыдущий ход агента \n",
    "    не выиграл (или ничья). В противном случае агент повторяет предыдущий ход.\n",
    "\n",
    "    Returns:\n",
    "        function: Функция `reactionary`, которая принимает `observation` и `configuration`\n",
    "                  и возвращает целое число (ход агента) от 0 до `configuration.signs - 1`.\n",
    "    \"\"\"\n",
    "    \n",
    "    move = None\n",
    "\n",
    "    def reactionary(observation, configuration):\n",
    "        \"\"\"\n",
    "        Определяет ход агента на основе последнего хода противника.\n",
    "\n",
    "        Args:\n",
    "            observation (object): Наблюдение о состоянии текущего раунда, включая:\n",
    "                                  - observation.step (int): текущий номер шага.\n",
    "                                  - observation.lastOpponentAction (int): жест, выбранный противником.\n",
    "            configuration (object): Конфигурация игры, включая:\n",
    "                                    - configuration.signs (int): количество возможных жестов.\n",
    "\n",
    "        Returns:\n",
    "            int: Выбранный жест агента, который может быть 0, 1 или 2.\n",
    "        \"\"\"\n",
    "        nonlocal move       \n",
    "        \n",
    "        if observation.step == 0:\n",
    "            move = random.randrange(0, configuration.signs)\n",
    "        elif get_score(move, observation.lastOpponentAction) <= 1:\n",
    "            move = (observation.lastOpponentAction + 1) % configuration.signs\n",
    "        \n",
    "        logging.info(\n",
    "            f\"reactionary: \"\n",
    "            f\"step={observation.step}, \"\n",
    "            f\"move={move}, \"\n",
    "            f\"reward={observation.reward}, \"\n",
    "        )\n",
    "        return move\n",
    "    return reactionary\n",
    "\n",
    "def create_counter_reactionary():\n",
    "    \"\"\"\n",
    "    Создает агента, который реагирует на ход противника, используя стратегию контр-реакции.\n",
    "\n",
    "    Агент делает случайный выбор на первом шаге. На последующих шагах:\n",
    "    - Если предыдущий ход агента побеждает ход противника, агент выбирает жест, который \n",
    "      контрит последний ход противника.\n",
    "    - В противном случае агент выбирает жест, который побеждает ход противника.\n",
    "\n",
    "    Returns:\n",
    "        function: Функция `counter_reactionary`, которая принимает `observation` и `configuration`\n",
    "                  и возвращает целое число (ход агента) от 0 до `configuration.signs - 1`.\n",
    "    \"\"\"\n",
    "    \n",
    "    move = None\n",
    "    \n",
    "    def counter_reactionary(observation, configuration):\n",
    "        \"\"\"\n",
    "        Определяет ход агента на основе последнего хода противника и предыдущего хода агента.\n",
    "\n",
    "        Args:\n",
    "            observation (object): Наблюдение о состоянии текущего раунда, включая:\n",
    "                                  - observation.step (int): текущий номер шага.\n",
    "                                  - observation.lastOpponentAction (int): жест, выбранный противником.\n",
    "            configuration (object): Конфигурация игры, включая:\n",
    "                                    - configuration.signs (int): количество возможных жестов.\n",
    "\n",
    "        Returns:\n",
    "            int: Выбранный жест агента, который может быть 0, 1 или 2.\n",
    "        \"\"\"\n",
    "        nonlocal move\n",
    "        \n",
    "        if observation.step == 0:\n",
    "            move = random.randrange(0, configuration.signs)\n",
    "        elif get_score(move, observation.lastOpponentAction) == 1:\n",
    "            move = (move + 2) % configuration.signs  \n",
    "        else:\n",
    "            move = (observation.lastOpponentAction + 1) % configuration.signs \n",
    "        \n",
    "        logging.info(\n",
    "            f\"counter_reactionary: \"\n",
    "            f\"step={observation.step}, \"\n",
    "            f\"move={move}, \"\n",
    "            f\"reward={observation.reward}, \"\n",
    "        )            \n",
    "        return move\n",
    "    return counter_reactionary\n",
    "\n",
    "def create_statistical():\n",
    "    \"\"\"\n",
    "    Создает агента, который реагирует на ход противника, основываясь на статистике предыдущих ходов.\n",
    "\n",
    "    Агент собирает историю ходов противника и выбирает жест, который контрит наиболее \n",
    "    часто используемый ход противника. На первом шаге он инициализирует историю.\n",
    "\n",
    "    Returns:\n",
    "        function: Функция `statistical`, которая принимает `observation` и `configuration`\n",
    "                  и возвращает целое число (ход агента) от 0 до `configuration.signs - 1`.\n",
    "    \"\"\"\n",
    "    \n",
    "    action_histogram = {}\n",
    "\n",
    "    def statistical(observation, configuration):\n",
    "        \"\"\"\n",
    "        Определяет ход агента на основе статистики предыдущих ходов противника.\n",
    "\n",
    "        Args:\n",
    "            observation (object): Наблюдение о состоянии текущего раунда, включая:\n",
    "                                  - observation.step (int): текущий номер шага.\n",
    "                                  - observation.lastOpponentAction (int): жест, выбранный противником.\n",
    "            configuration (object): Конфигурация игры, включая:\n",
    "                                    - configuration.signs (int): количество возможных жестов.\n",
    "\n",
    "        Returns:\n",
    "            int: Выбранный жест агента, который может быть 0, 1 или 2.\n",
    "        \"\"\"\n",
    "        nonlocal action_histogram\n",
    "        \n",
    "        if observation.step == 0:\n",
    "            action_histogram = {}  \n",
    "            return  \n",
    "        \n",
    "        action = observation.lastOpponentAction\n",
    "        \n",
    "        if action not in action_histogram:\n",
    "            action_histogram[action] = 0\n",
    "        action_histogram[action] += 1\n",
    "\n",
    "        mode_action = None\n",
    "        mode_action_count = None\n",
    "        \n",
    "        for k, v in action_histogram.items():\n",
    "            if mode_action_count is None or v > mode_action_count:\n",
    "                mode_action = k\n",
    "                mode_action_count = v\n",
    "     \n",
    "        move = (mode_action + 1) % configuration.signs   \n",
    "        \n",
    "        logging.info(\n",
    "            f\"statistical: \"\n",
    "            f\"step={observation.step}, \"\n",
    "            f\"move={move}, \"\n",
    "            f\"reward={observation.reward}, \"\n",
    "        )             \n",
    "        return move\n",
    "    return statistical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итого 15 агентов, запустим их в турнир. Всего 5 раундов, каждый матч по 200 ходов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant_rock: 5 побед\n",
      "constant_paper: 5 побед\n",
      "constant_scissors: 10 побед\n",
      "sequence: 10 побед\n",
      "random_sign: 4 побед\n",
      "copy_opponent: 20 побед\n",
      "respond_to_opponent: 26 побед\n",
      "rotation: 30 побед\n",
      "common_selection: 10 побед\n",
      "reward_check: 27 побед\n",
      "anti_replay: 29 побед\n",
      "predictive_agent: 27 побед\n",
      "reactionary: 27 побед\n",
      "counter_reactionary: 41 побед\n",
      "statistical: 23 побед\n"
     ]
    }
   ],
   "source": [
    "env = make(\"rps\")\n",
    "agents = [\n",
    "    constant_rock,\n",
    "    constant_paper,\n",
    "    constant_scissors,\n",
    "    create_sequence(), \n",
    "    random_sign,\n",
    "    copy_opponent,\n",
    "    respond_to_opponent, \n",
    "    create_rotation(), \n",
    "    create_common_selection(),\n",
    "    create_reward_check(),\n",
    "    create_anti_replay(),\n",
    "    create_predictive_agent(),\n",
    "    create_reactionary(),\n",
    "    create_counter_reactionary(),\n",
    "    create_statistical()\n",
    "]\n",
    "results = {agent.__name__: 0 for agent in agents}\n",
    "match_results = [] \n",
    "\n",
    "for i in range(len(agents)):\n",
    "    for j in range(i + 1, len(agents)):\n",
    "        agent1, agent2 = agents[i], agents[j]\n",
    "        \n",
    "        for match in range(5):\n",
    "            rewards = evaluate(\n",
    "                \"rps\",\n",
    "                [agent1, agent2],\n",
    "                configuration={\"episodeSteps\": 200},\n",
    "                debug=False\n",
    "            )[0]\n",
    "\n",
    "            match_results.append((\n",
    "                agent1.__name__, \n",
    "                agent2.__name__, \n",
    "                rewards[0], \n",
    "                rewards[1]\n",
    "            ))\n",
    "\n",
    "            if rewards[0] > rewards[1]:\n",
    "                results[agent1.__name__] += 1 \n",
    "            elif rewards[1] > rewards[0]:\n",
    "                results[agent2.__name__] += 1 \n",
    "\n",
    "for agent_name, score in results.items():\n",
    "    print(f\"{agent_name}: {score} побед\")\n",
    "\n",
    "df_rewards = pd.DataFrame(\n",
    "    match_results, columns=[\n",
    "        \"Agent 1\", \n",
    "        \"Agent 2\", \n",
    "        \"Rewards Agent 1\", \n",
    "        \"Rewards Agent 2\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь посмотрим на среднее значение reward по агентам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Agent</th>\n",
       "      <th>Average Reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>counter_reactionary</td>\n",
       "      <td>64.942857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>anti_replay</td>\n",
       "      <td>34.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>predictive_agent</td>\n",
       "      <td>28.871429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>reactionary</td>\n",
       "      <td>28.014286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rotation</td>\n",
       "      <td>23.157143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>respond_to_opponent</td>\n",
       "      <td>22.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>copy_opponent</td>\n",
       "      <td>18.057143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sequence</td>\n",
       "      <td>17.614286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>statistical</td>\n",
       "      <td>15.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>random_sign</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>reward_check</td>\n",
       "      <td>-22.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>common_selection</td>\n",
       "      <td>-41.614286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>constant_scissors</td>\n",
       "      <td>-49.371429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>constant_paper</td>\n",
       "      <td>-62.628571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>constant_rock</td>\n",
       "      <td>-77.414286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Agent  Average Reward\n",
       "13  counter_reactionary       64.942857\n",
       "10          anti_replay       34.200000\n",
       "11     predictive_agent       28.871429\n",
       "12          reactionary       28.014286\n",
       "7              rotation       23.157143\n",
       "6   respond_to_opponent       22.571429\n",
       "5         copy_opponent       18.057143\n",
       "3              sequence       17.614286\n",
       "14          statistical       15.200000\n",
       "4           random_sign        0.500000\n",
       "9          reward_check      -22.100000\n",
       "8      common_selection      -41.614286\n",
       "2     constant_scissors      -49.371429\n",
       "1        constant_paper      -62.628571\n",
       "0         constant_rock      -77.414286"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_list = []\n",
    "agents = [\n",
    "    'constant_rock',\n",
    "    'constant_paper',\n",
    "    'constant_scissors',\n",
    "    'sequence',\n",
    "    'random_sign',\n",
    "    'copy_opponent',\n",
    "    'respond_to_opponent',\n",
    "    'rotation',\n",
    "    'common_selection',\n",
    "    'reward_check',\n",
    "    'anti_replay',\n",
    "    'predictive_agent',\n",
    "    'reactionary',\n",
    "    'counter_reactionary',\n",
    "    'statistical'\n",
    "]\n",
    "\n",
    "for agent in agents:\n",
    "    rewards_agent1 = df_rewards[df_rewards['Agent 1'] == agent]['Rewards Agent 1']\n",
    "    rewards_agent2 = df_rewards[df_rewards['Agent 2'] == agent]['Rewards Agent 2']\n",
    "    all_rewards = pd.concat([rewards_agent1, rewards_agent2])\n",
    "    average_reward = all_rewards.mean()\n",
    "    results_list.append({'Agent': agent, 'Average Reward': average_reward})\n",
    "\n",
    "final_results = pd.DataFrame(results_list)\n",
    "final_results.sort_values(by='Average Reward', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вывод<br>\n",
    "**Топ агентов по количеству побед:**\n",
    "- counter_reactionary: 41 \n",
    "- rotation: 30\n",
    "- anti_replay: 29\n",
    "- reactionary: 27\n",
    "- reward_check: 27\n",
    "- predictive_agent: 27<br><br>\n",
    "\n",
    "**Топ агентов по среднему reward:**\n",
    "- counter_reactionary: 64.94\n",
    "- anti_replay: 34.2\n",
    "- predictive_agent:\t28.87\n",
    "- reactionary: 28.01\n",
    "- rotation: 23.15<br><br>\n",
    "\n",
    "1. Уверенную победу одерживает `counter_reactionary`, придерживающийся тактики противодействия последнему ходу противника и повторяющий свой выигрышный ход. Именно эта особенность отличает его от `reactionary` и `respond_to_opponent` и даёт ему преимущество по сравнению с ними в наборе с большим количеством агентов так или иначе выставляющих свои ходы случайным образом.<br><br>\n",
    "2. Агенты `rotation` и `anti_replay`, выставляющие ходы рандомно, но избегающие слишком частых повторений, также продемонстрировали высокую результативность.<br><br>\n",
    "3. Агенты, опирающиеся на статистику, в данном наборе оказались не слишком эффективны, так как рандомная генерация ходов создаёт примерно одинаковую вероятность каждого хода и затрудняет предсказание следующего хода."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mipt2)",
   "language": "python",
   "name": "mipt2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
