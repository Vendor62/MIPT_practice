{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d3796a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from pymorphy3 import MorphAnalyzer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "dfc853c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"D:\\\\ds\\\\jokes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f55970",
   "metadata": {},
   "source": [
    "Напишите функцию для предобработки текста:\n",
    "- Удаление пунктуационных знаков (символы возьмите из модуля string)\n",
    "- Используйте кеш для хранения нормальных форм слов\n",
    "- Лемматизируйте слова\n",
    "\n",
    "Запустите функцию на десятом по индексу объекте из датасета и укажите нулевое слово."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2c90c51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Нулевое слово: лететь\n"
     ]
    }
   ],
   "source": [
    "# Инициализация лемматизатора\n",
    "morph = MorphAnalyzer()\n",
    "\n",
    "# Кэш для хранения нормальных форм слов \n",
    "cache = {}\n",
    "\n",
    "def preprocess_txt(line):\n",
    "    punctuation = string.punctuation  # пунктуационные знаки из модуля string\n",
    "    tokens = \"\".join(i.lower() for i in line.strip() if i not in punctuation).split()\n",
    "    \n",
    "    words_lem = []\n",
    "    for w in tokens:\n",
    "        if w in cache:\n",
    "            words_lem.append(cache[w])  # добавление норм. формы из кэша\n",
    "        else:\n",
    "            temp_cach = cache[w] = morph.parse(w)[0].normal_form  # получение норм. формы\n",
    "            words_lem.append(temp_cach)\n",
    "    \n",
    "    return ' '.join(words_lem)\n",
    "\n",
    "# Пример использования (предполагая, что df - ваш DataFrame)\n",
    "text = df.loc[9, 'text']  # десятый объект (индекс 9)\n",
    "processed_text = preprocess_txt(text)\n",
    "first_word = processed_text.split()[0] if processed_text else \"Текст пуст\"\n",
    "\n",
    "print(f\"Нулевое слово: {first_word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5efb889",
   "metadata": {},
   "source": [
    "1. Сделайте разбиение на обучение и тест с соотношением 80/20 и random_state=1.\n",
    "\n",
    "2. Запустите функцию preprocess_txt на тренировочной и тестовой выборках\n",
    "\n",
    "3. Соедините слова из тренировочной выборки в единый текст в переменную text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "185bebcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текст из тренировочной выборки (объединенные слова): моргот трындец похоже в жизнь прийтись что то пересматривать gemoroy что случиться моргот нет ты прикинуть я блэкушник с десятилетний стаж моргот обладатель чумовой гроул моргот да когда я говорить собака разбегаться от мой рёв моргот и всё это уйти в никуда сейчас моргот когда по телефон после мой аллла какой то старуха попросить девочка принять факс принимать както василий иванович в партия он к котовский помочь что говорить на партсобрание ну как там ты вопрос быть задавать ты выучить устать...\n"
     ]
    }
   ],
   "source": [
    "# Разбиение на обучающую и тестовую выборки (80/20)\n",
    "X_train, X_test = train_test_split(df['text'], test_size=0.2, random_state=1)\n",
    "\n",
    "# Применяем функцию preprocess_txt к тренировочной выборке\n",
    "train_processed = X_train.apply(preprocess_txt)\n",
    "\n",
    "# Применяем функцию preprocess_txt к тестовой выборке\n",
    "test_processed = X_test.apply(preprocess_txt)\n",
    "\n",
    "# Функция для фильтрации пустых значений и преобразования в строку\n",
    "def filter_and_join(text_series):\n",
    "    # Преобразуем список в строку, игнорируя NaN и пустые значения\n",
    "    return ' '.join(str(word) for word in text_series.explode() if isinstance(word, str))\n",
    "\n",
    "# Объединяем все лемматизированные слова из тренировочной выборки в единую строку\n",
    "text = filter_and_join(train_processed)\n",
    "\n",
    "# Выводим результат\n",
    "print(f\"Текст из тренировочной выборки (объединенные слова): {text[:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0516af7",
   "metadata": {},
   "source": [
    "4. Посчитайте частотность появления каждого слова в тексте и сформируйте словарь count_words\n",
    "\n",
    "5. Укажите частоту появления слова молодец\n",
    "\n",
    "6. Запишите в список my_stopwords слова, которые встречаются чаще 1000 раз и укажите размер этого списка. Он пригодится чуть позже.\n",
    "\n",
    "Внимание! В поле ответа внесите через пробел два числа, первое из которых указывает частоту появления слова молодец, второе - размер списка my_stopwords, например, 100 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc9bffe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Частота слова 'молодец': 195\n",
      "Размер списка my_stopwords: 98\n"
     ]
    }
   ],
   "source": [
    "all_words = train_processed.explode().tolist()\n",
    "\n",
    "# Создаем словарь с подсчетом частоты каждого слова\n",
    "count_words = Counter(all_words)\n",
    "\n",
    "# Частота появления слова \"молодец\"\n",
    "frequency_molodets = count_words.get('молодец', 0)\n",
    "\n",
    "# Формируем список стоп-слов, встречающихся более 1000 раз\n",
    "my_stopwords = [word for word, count in count_words.items() if count > 1000]\n",
    "\n",
    "# Размер списка стоп-слов\n",
    "stopwords_size = len(my_stopwords)\n",
    "\n",
    "# Выводим результат\n",
    "print(f\"Частота слова 'молодец': {frequency_molodets}\")\n",
    "print(f\"Размер списка my_stopwords: {stopwords_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7f9449",
   "metadata": {},
   "source": [
    "- Инициализируйте объект CountVectorizer c max_features=10000.\n",
    "- Обучите векторизатор на данных после применения функции preprocess_txt.\n",
    "- Преобразуйте тренировочную и тестовую выборки.\n",
    "- Обучите модель LogisticRegression.\n",
    "- Укажите, какая метрика accuracy получилась, округлив значение до второго знака после точки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c549946f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.68\n"
     ]
    }
   ],
   "source": [
    "def preprocess_txt(line):\n",
    "    punctuation = string.punctuation  # Пунктуационные знаки\n",
    "    # Удаляем знаки препинания и приводим текст к нижнему регистру\n",
    "    tokens = \"\".join(i.lower() for i in line.strip() if i not in punctuation).split()\n",
    "    \n",
    "    words_lem = []\n",
    "    for w in tokens:\n",
    "        if w in cache:\n",
    "            # Если слово в кэше, используем его нормальную форму\n",
    "            words_lem.append(cache[w])\n",
    "        else:\n",
    "            # Лемматизируем слово и добавляем в кэш\n",
    "            temp_cach = cache[w] = morph.parse(w)[0].normal_form\n",
    "            words_lem.append(temp_cach)\n",
    "    \n",
    "    # Возвращаем текст с лемматизированными словами\n",
    "    return ' '.join(words_lem)\n",
    "\n",
    "# Применяем функцию к 10-му объекту (индекс 9)\n",
    "text = df.loc[9, 'text']  # берем текст с индексом 9\n",
    "preprocessed_tokens = preprocess_txt(text)\n",
    "\n",
    "# Разделим на признаки и целевую переменную\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['rating'], test_size=0.2, random_state=1)\n",
    "\n",
    "# Применяем функцию preprocess_txt к тренировочной и тестовой выборкам\n",
    "X_train_processed = X_train.apply(preprocess_txt)\n",
    "X_test_processed = X_test.apply(preprocess_txt)\n",
    "\n",
    "# Инициализируем CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=10000)\n",
    "\n",
    "# Обучаем векторизатор и трансформируем данные\n",
    "X_train_vec = vectorizer.fit_transform(X_train_processed)\n",
    "X_test_vec = vectorizer.transform(X_test_processed)\n",
    "\n",
    "# Обучаем логистическую регрессию\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "# Предсказание и оценка\n",
    "y_pred = model.predict(X_test_vec)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7126977",
   "metadata": {},
   "source": [
    "Стоп-слова из библиотеке nltk:\n",
    "- Скачайте стоп-слова для русского языка из библиотеки nltk.\n",
    "- Напишите функцию del_stopwords(line, stopwords) для удаления стоп-слов, на вход функции поступает текст и список стоп-слов, на выходе функции текст без стоп-слов.\n",
    "- Удалите стоп-слова из тренировочной и тестовой выборках.\n",
    "- Инициализируйте объект CountVectorizer c max_features=10000.\n",
    "- Обучите векторизатор на данных после применения функции preprocess_txt и после удаления стоп-слов.\n",
    "- Преобразуйте тренировочную и тестовую выборки.\n",
    "- Обучите модель LogisticRegression.\n",
    "- Посчитайте метрику accuracy.\n",
    "\n",
    "Стоп-слова из задания 2:\n",
    "- Используйте список стоп-слов my_stopwords из задания 2, удалите их из тренировочной и тестовой выборок.\n",
    "- Инициализируйте объект CountVectorizer c max_features=10000.\n",
    "- Обучите векторизатор на данных после применения функции preprocess_txt и после удаления стоп-слов.\n",
    "- Преобразуйте тренировочную и тестовую выборки.\n",
    "- Обучите модель LogisticRegression.\n",
    "- Посчитайте метрику accuracy.\n",
    "- Укажите разницу в метриках, округлив её до второго знака после точки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cde89c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "russian_stopwords = stopwords.words(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4cf50365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_stopwords(line, stopwords_list):\n",
    "    return ' '.join([word for word in line.split() if word not in stopwords_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28634f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предобработка и удаление nltk-стопслов\n",
    "X_train_processed_nltk = X_train.apply(preprocess_txt).apply(lambda x: del_stopwords(x, russian_stopwords))\n",
    "X_test_processed_nltk = X_test.apply(preprocess_txt).apply(lambda x: del_stopwords(x, russian_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7727f1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (nltk stopwords): 0.68\n"
     ]
    }
   ],
   "source": [
    "vectorizer_nltk = CountVectorizer(max_features=10000)\n",
    "X_train_vec_nltk = vectorizer_nltk.fit_transform(X_train_processed_nltk)\n",
    "X_test_vec_nltk = vectorizer_nltk.transform(X_test_processed_nltk)\n",
    "\n",
    "model_nltk = LogisticRegression(max_iter=1000)\n",
    "model_nltk.fit(X_train_vec_nltk, y_train)\n",
    "y_pred_nltk = model_nltk.predict(X_test_vec_nltk)\n",
    "accuracy_nltk = accuracy_score(y_test, y_pred_nltk)\n",
    "print(f\"Accuracy (nltk stopwords): {accuracy_nltk:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c00bd905",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stopwords = [word for word, freq in count_words.items() if freq > 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ee59b7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (my stopwords): 0.68\n"
     ]
    }
   ],
   "source": [
    "X_train_processed_my = X_train.apply(preprocess_txt).apply(lambda x: del_stopwords(x, my_stopwords))\n",
    "X_test_processed_my = X_test.apply(preprocess_txt).apply(lambda x: del_stopwords(x, my_stopwords))\n",
    "\n",
    "vectorizer_my = CountVectorizer(max_features=10000)\n",
    "X_train_vec_my = vectorizer_my.fit_transform(X_train_processed_my)\n",
    "X_test_vec_my = vectorizer_my.transform(X_test_processed_my)\n",
    "\n",
    "model_my = LogisticRegression(max_iter=1000)\n",
    "model_my.fit(X_train_vec_my, y_train)\n",
    "y_pred_my = model_my.predict(X_test_vec_my)\n",
    "accuracy_my = accuracy_score(y_test, y_pred_my)\n",
    "print(f\"Accuracy (my stopwords): {accuracy_my:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1e420ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Разница метрик: 0.0\n"
     ]
    }
   ],
   "source": [
    "delta = round(abs(accuracy_nltk - accuracy_my), 2)\n",
    "print(f\"Разница метрик: {delta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9655c0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b394492",
   "metadata": {},
   "source": [
    "Целевой признак cyberbullying_type\n",
    "\n",
    "Создайте объект класса WordNetLemmatizer\n",
    "\n",
    "Напишите функцию preprocess_txt для предобработки текста:\n",
    "\n",
    "Удаление хештегов и упоминание.\n",
    "\n",
    "Удаление пунктуационных знаков (символы возьмите из модуля string).\n",
    "\n",
    "Лемматизируйте слова.\n",
    "\n",
    "Удалите стоп-слова из библиотеки nltk.\n",
    "\n",
    "Можете воспользоваться следующей заготовкой.\n",
    "\n",
    "Сделайте разбиение на обучение и тест с соотношением 80/20 и random_state=1.\n",
    "\n",
    "Запустите функцию preprocess_txt на тренировочной и тестовой выборках.\n",
    "\n",
    "Посчитайте кол-во слов в каждом тексте после преобразования и удалите строки, если размер равен 0.\n",
    "\n",
    "Инициализируйте объект TfidfVectorizer c max_features=10000.\n",
    "\n",
    "Обучите векторизатор на данных после применения функции preprocess_txt.\n",
    "\n",
    "Преобразуйте тренировочную и тестовую выборки.\n",
    "\n",
    "Обучите модель LogisticRegression.\n",
    "\n",
    "Укажите, какая метрика accuracy получилась, округлив её до второго знака после точки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f3dbefaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"D:\\\\ds\\\\cyberbullying_tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "11559f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.81\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "X = df['tweet_text']  # Тексты\n",
    "y = df['cyberbullying_type']  # Целевая переменная\n",
    "\n",
    "# Создаем объект WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Функция предобработки текста\n",
    "def preprocess_txt(text):\n",
    "    # Удаление хештегов и упоминаний\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # Удаление пунктуации\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Лемматизация\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    # Удаление стоп-слов\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word.lower() not in stop_words]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "# Разбиение на обучение и тест (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Применяем предобработку\n",
    "X_train_processed = X_train.apply(preprocess_txt)\n",
    "X_test_processed = X_test.apply(preprocess_txt)\n",
    "\n",
    "# Удаляем пустые тексты\n",
    "train_non_empty = X_train_processed.str.len() > 0\n",
    "X_train_processed = X_train_processed[train_non_empty]\n",
    "y_train = y_train[train_non_empty]\n",
    "\n",
    "test_non_empty = X_test_processed.str.len() > 0\n",
    "X_test_processed = X_test_processed[test_non_empty]\n",
    "y_test = y_test[test_non_empty]\n",
    "\n",
    "# Векторизация (TF-IDF)\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X_train_vec = vectorizer.fit_transform(X_train_processed)\n",
    "X_test_vec = vectorizer.transform(X_test_processed)\n",
    "\n",
    "# Обучение модели\n",
    "model = LogisticRegression(max_iter=1000)  # Увеличиваем max_iter для сходимости\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "# Предсказание и оценка\n",
    "y_pred = model.predict(X_test_vec)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da5a2c4",
   "metadata": {},
   "source": [
    "Обучите модель Word2Vec из модуля gensim:\n",
    "- Возьмите размер эмбеддинга 100.\n",
    "- Минимальное количество появления слова 2.\n",
    "- Окно размера 5.\n",
    "- Для воспроизводимости зафиксируйте параметры seed=1, workers=1.\n",
    "- Выведите эмбеддинг для слова cat и укажите нулевое значение, округлив его до второго знака после точки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3777c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Нулевое значение эмбеддинга 'cat': -0.15000000596046448\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Разбиваем предобработанные тексты на списки слов\n",
    "sentences = [text.split() for text in X_train_processed]\n",
    "\n",
    "# Обучаем Word2Vec\n",
    "model_w2v = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=100,       # Размер эмбеддинга\n",
    "    window=5,              # Размер окна\n",
    "    min_count=2,           # Минимальное количество появления слова\n",
    "    seed=1,                # Для воспроизводимости\n",
    "    workers=1              # Количество потоков\n",
    ")\n",
    "\n",
    "# Получаем эмбеддинг для слова \"cat\"\n",
    "try:\n",
    "    embedding_cat = model_w2v.wv[\"cat\"]\n",
    "    print(f\"Нулевое значение эмбеддинга 'cat': {round(embedding_cat[0], 2)}\")\n",
    "except KeyError:\n",
    "    print(\"Слово 'cat' отсутствует в словаре Word2Vec.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9324ccbd",
   "metadata": {},
   "source": [
    "Реализуйте функцию get_mean_embedding для подсчета среднего эмбеддинга для всего предложения:\n",
    "- Получите размер эмбеддинга из атрибута модели.\n",
    "- Суммируйте значения эмбеддингов.\n",
    "- Усредните значения эмбеддингов по кол-ву слов, которые есть в модели Word2Vec.\n",
    "- Получите усредненный эмбеддинг для тренировочной и тестовой выборок.\n",
    "- Обучите модель LogisticRegression.\n",
    "- Укажите, какая метрика accuracy получилась, округлив значение до второго знака после точки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "af537717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Word2Vec + LogisticRegression): 0.73\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def get_mean_embedding(sent, model):\n",
    "    vector_size = model.vector_size  # получение размера эмбеддинга из модели\n",
    "    model_res = np.zeros(vector_size)  # заготовка для усредненного эмбеддинга\n",
    "    ctr = 0  # счетчик слов\n",
    "    for word in sent.split():  # разбиваем предложение на слова\n",
    "        if word in model.wv:  # проверяем, есть ли слово в модели\n",
    "            model_res += model.wv[word]  # добавление эмбеддинга\n",
    "            ctr += 1\n",
    "    if ctr > 0:\n",
    "        model_res /= ctr  # усреднение эмбеддинга по словам\n",
    "    return model_res\n",
    "\n",
    "# Получаем усредненные эмбеддинги для тренировочной и тестовой выборок\n",
    "X_train_w2v = np.array([get_mean_embedding(text, model_w2v) for text in X_train_processed])\n",
    "X_test_w2v = np.array([get_mean_embedding(text, model_w2v) for text in X_test_processed])\n",
    "\n",
    "# Удаляем строки, где не нашлось ни одного слова из Word2Vec\n",
    "train_non_empty = np.array([len(x) > 0 for x in X_train_w2v])\n",
    "X_train_w2v = X_train_w2v[train_non_empty]\n",
    "y_train_w2v = y_train[train_non_empty]\n",
    "\n",
    "test_non_empty = np.array([len(x) > 0 for x in X_test_w2v])\n",
    "X_test_w2v = X_test_w2v[test_non_empty]\n",
    "y_test_w2v = y_test[test_non_empty]\n",
    "\n",
    "# Обучаем LogisticRegression на эмбеддингах Word2Vec\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=1)\n",
    "lr_model.fit(X_train_w2v, y_train_w2v)\n",
    "\n",
    "# Предсказание и оценка качества\n",
    "y_pred_w2v = lr_model.predict(X_test_w2v)\n",
    "accuracy_w2v = accuracy_score(y_test_w2v, y_pred_w2v)\n",
    "\n",
    "print(f\"Accuracy (Word2Vec + LogisticRegression): {accuracy_w2v:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa28be81",
   "metadata": {},
   "source": [
    "Реализуйте функцию tokenize для токенизации текстов с входными параметрами:\n",
    "- sents - предложения.\n",
    "- seq_len - длина текста.\n",
    "- Пройдитесь в цикле по текстам.\n",
    "- Пройдитесь в цикле по словам.\n",
    "- Если слово есть в словаре, то получите его индекс и добавьте в список.\n",
    "- Если у текста длина меньше seq_len, нужно ее заполнить токеном PAD.\n",
    "- Запустите функцию tokenize на предобработанных (после функции preprocess_txt) тренировочной и тестовой выборках с максимальной длиной последовательности 50\n",
    "- Укажите через пробел индексы последних трех слов в нулевом тренировочном тексте. Например, 20 4000 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b8344efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Индексы последних трех слов: 21 9037 811\n"
     ]
    }
   ],
   "source": [
    "# Создаем словарь модели с токеном PAD\n",
    "vocab = {k: v+1 for k, v in model_w2v.wv.key_to_index.items()}\n",
    "vocab['PAD'] = 0\n",
    "\n",
    "def tokenize(sents, seq_len):\n",
    "    # Перевод текстов в численный вид\n",
    "    text_int = []\n",
    "    for text in sents:  # проход по текстам\n",
    "        r = []\n",
    "        for word in text.split():  # проход по словам в тексте\n",
    "            if word in vocab:  # если слово есть в словаре\n",
    "                r.append(vocab[word])  # добавляем индекс слова\n",
    "        text_int.append(r)\n",
    "\n",
    "    # Добавление паддинга\n",
    "    features = np.zeros((len(text_int), seq_len), dtype=int)\n",
    "    for i, text in enumerate(text_int):\n",
    "        if len(text) < seq_len:  # если размер текста меньше seq_len\n",
    "            # Добавляем PAD токены в начало\n",
    "            new = [vocab['PAD']] * (seq_len - len(text)) + text\n",
    "        else:\n",
    "            # Урезаем текст до seq_len\n",
    "            new = text[:seq_len]\n",
    "        features[i, :] = np.array(new)\n",
    "\n",
    "    return features\n",
    "\n",
    "# Применяем токенизацию с seq_len=50\n",
    "X_train_tokenized = tokenize(X_train_processed, seq_len=50)\n",
    "X_test_tokenized = tokenize(X_test_processed, seq_len=50)\n",
    "\n",
    "# Получаем индексы последних трех слов в нулевом тексте\n",
    "last_three_indices = X_train_tokenized[0][-3:]\n",
    "\n",
    "print(\"Индексы последних трех слов:\", ' '.join(map(str, last_three_indices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340f00df",
   "metadata": {},
   "source": [
    "Сформируйте матрицу эмбеддингов, где каждая строка будет из себя представлять эмбеддинг. Размер матрицы должен быть (16682, 100).\n",
    "\n",
    "Преобразуйте строковое обозначение классов в порядковые числа через LabelEncoder\n",
    "\n",
    "Создайте TensorDataset из массива индексов слов для выборки тренировочной и тестовой.\n",
    "\n",
    "Создайте DataLoader для обучения и теста. Используйте перемешивание и batch_size=128.\n",
    "\n",
    "Сделайте одну итерацию в цикле по DataLoader для обучения и укажите два последних индекса слов для нулевого объекта. Для воспроизводимости используйте torch.manual_seed(1)\n",
    "Внимание! В поле ответа через пробел внесите два последних индекса слов для нулевого объекта, например, 2000 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a1cabbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2698 615\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Формируем матрицу эмбеддингов (16682, 100)\n",
    "embedding_matrix = np.zeros((len(vocab), 100))  # +1 для PAD токена\n",
    "for word, idx in vocab.items():\n",
    "    if word != 'PAD' and word in model_w2v.wv:\n",
    "        embedding_matrix[idx] = model_w2v.wv[word]\n",
    "\n",
    "# Преобразуем строковые классы в числовые\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Создаем TensorDataset\n",
    "train_dataset = TensorDataset(torch.LongTensor(X_train_tokenized), \n",
    "                             torch.LongTensor(y_train_encoded))\n",
    "test_dataset = TensorDataset(torch.LongTensor(X_test_tokenized), \n",
    "                            torch.LongTensor(y_test_encoded))\n",
    "\n",
    "# Создаем DataLoader с перемешиванием\n",
    "torch.manual_seed(1)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# Одна итерация по DataLoader\n",
    "for batch in train_loader:\n",
    "    inputs, labels = batch\n",
    "    # Получаем два последних индекса для первого объекта в батче\n",
    "    last_two_indices = inputs[0][-2:].tolist()\n",
    "    print(' '.join(map(str, last_two_indices)))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b326f6d1",
   "metadata": {},
   "source": [
    "Создайте класс нейронной сети Classifier, который наследуется от nn.Module на классификацию на 6 классов:\n",
    "- со слоем Embedding размерностью 100\n",
    "- с фиксированной случайностью torch.manual_seed(1)\n",
    "- с усреднением эмбеддингов слов в предложении\n",
    "\n",
    "Укажите, какое предсказание получается у необученной нейросети на нулевом тренировочном объекте на нулевой класс."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "db59db6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вероятность нулевого класса: 0.1412\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_out):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1)  # Фиксация случайности\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc = nn.Linear(embedding_dim, n_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len]\n",
    "        embedded_words = self.embedding(x)  # [batch_size, seq_len, embedding_dim]\n",
    "        embedded_sent = torch.mean(embedded_words, dim=1)  # Усреднение по словам [batch_size, embedding_dim]\n",
    "        out = self.fc(embedded_sent)  # [batch_size, n_out]\n",
    "        return out\n",
    "\n",
    "# Параметры модели\n",
    "vocab_size = len(vocab)  # Размер словаря (включая PAD)\n",
    "embedding_dim = 100      # Размерность эмбеддингов\n",
    "n_out = 6                # Количество классов\n",
    "\n",
    "# Создаем модель\n",
    "model = Classifier(vocab_size, embedding_dim, n_out)\n",
    "\n",
    "# Получаем предсказание для нулевого объекта\n",
    "with torch.no_grad():\n",
    "    sample_input = torch.LongTensor(X_train_tokenized[:1])  # Берем первый объект\n",
    "    prediction = model(sample_input)\n",
    "    class_0_prob = torch.softmax(prediction, dim=1)[0, 0].item()\n",
    "\n",
    "print(f\"Вероятность нулевого класса: {class_0_prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adeb280a",
   "metadata": {},
   "source": [
    "В этом задании будете обучать только один линейный слой на классификацию текстов по эмбеддингам из модели Word2Vec. Для этого замените веса в слое Embedding на веса из матрицы embedding_matrix. Укажите для этого слоя requires_grad = False\n",
    "\n",
    "Запустите обучение:\n",
    "\n",
    "Оптимизатор Adam со скоростью обучения 0.001.\n",
    "\n",
    "Количество эпох 10.\n",
    "\n",
    "Укажите, какая метрика accuracy получается на последней эпохе на тестовой выборке, округлив значение до второго знака после точки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f62fcbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [00:00<00:00, 396.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Train Loss: 0.011 - Train Acc: 0.593 Test Loss: 0.010 - Test Acc: 0.651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [00:00<00:00, 385.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 Train Loss: 0.009 - Train Acc: 0.674 Test Loss: 0.008 - Test Acc: 0.691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [00:00<00:00, 382.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 Train Loss: 0.007 - Train Acc: 0.703 Test Loss: 0.007 - Test Acc: 0.719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [00:00<00:00, 384.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 Train Loss: 0.007 - Train Acc: 0.720 Test Loss: 0.006 - Test Acc: 0.729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [00:00<00:00, 352.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 Train Loss: 0.006 - Train Acc: 0.729 Test Loss: 0.006 - Test Acc: 0.735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [00:00<00:00, 409.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 Train Loss: 0.006 - Train Acc: 0.735 Test Loss: 0.006 - Test Acc: 0.735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [00:00<00:00, 353.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 Train Loss: 0.006 - Train Acc: 0.737 Test Loss: 0.006 - Test Acc: 0.737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [00:00<00:00, 382.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 Train Loss: 0.005 - Train Acc: 0.739 Test Loss: 0.005 - Test Acc: 0.738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [00:00<00:00, 384.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 Train Loss: 0.005 - Train Acc: 0.739 Test Loss: 0.005 - Test Acc: 0.740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [00:00<00:00, 344.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 Train Loss: 0.005 - Train Acc: 0.742 Test Loss: 0.005 - Test Acc: 0.741\n",
      "\n",
      "Final Test Accuracy: 0.74\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torch import nn, optim\n",
    "\n",
    "def train_model(model, opt, train_loader, test_loader, n_epoch=100):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    acc_train_list, acc_test_list = [], []\n",
    "\n",
    "    for epoch_num in range(n_epoch):\n",
    "        train_loss, test_loss = 0.0, 0.0\n",
    "        correct_train, correct_test = 0, 0\n",
    "        total_train, total_test = 0, 0\n",
    "\n",
    "        for data, labels in tqdm(train_loader):\n",
    "            opt.zero_grad()  # обнуление градиентов\n",
    "            y_predicted = model(data)  # получение предсказаний\n",
    "            loss = loss_fn(y_predicted, labels)  # подсчет функции потерь\n",
    "\n",
    "            loss.backward()  # обратный проход\n",
    "            opt.step()  # шаг оптимизации\n",
    "\n",
    "            # фиксация функции потерь и accuracy на обучении\n",
    "            train_loss += loss.item()  # суммируем ошибку\n",
    "            total_train += len(labels)  # суммируем кол-во объектов\n",
    "            _, predicted = torch.max(y_predicted, 1)  # получаем предсказанные классы\n",
    "            correct_train += (predicted == labels).sum().item()  # суммируем кол-во правильных классификаций\n",
    "\n",
    "        # вычисляем значение функции потерь и метрик на обучении на всем train_loader\n",
    "        train_acc = correct_train / total_train\n",
    "        train_loss = train_loss / total_train\n",
    "        acc_train_list.append(train_acc)\n",
    "\n",
    "\n",
    "        for data, labels in test_loader:\n",
    "            outputs = model(data)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            # фиксация функции потерь и accuracy на тесте\n",
    "            test_loss += loss.item()  # суммируем ошибку\n",
    "            total_test += len(labels)  # суммируем кол-во объектов\n",
    "            _, predicted = torch.max(outputs.data, 1)  # получаем предсказанные классы\n",
    "            correct_test += (predicted == labels).sum().item()  # суммируем кол-во правильных классификаций\n",
    "\n",
    "        # вычисляем значение функции потерь и метрик на тесте на всем test_loader\n",
    "        test_acc = correct_test / total_test\n",
    "        test_loss = test_loss / total_test\n",
    "\n",
    "        acc_test_list.append(test_acc)\n",
    "\n",
    "\n",
    "        # печатаем результаты эпохи\n",
    "        print(f\"Epoch {epoch_num+1}/{n_epoch} \" \\\n",
    "              f\"Train Loss: {train_loss:.3f} - Train Acc: {train_acc:.3f} \" \\\n",
    "              f\"Test Loss: {test_loss:.3f} - Test Acc: {test_acc:.3f}\")\n",
    "\n",
    "    return model, acc_train_list, acc_test_list \n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_out, embedding_matrix):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1)\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # Загружаем предобученные веса и замораживаем их\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.fc = nn.Linear(embedding_dim, n_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded_words = self.embedding(x)\n",
    "        embedded_sent = torch.mean(embedded_words, dim=1)\n",
    "        return self.fc(embedded_sent)\n",
    "\n",
    "# 2. Инициализация модели\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 100\n",
    "n_out = 6\n",
    "model = Classifier(vocab_size, embedding_dim, n_out, embedding_matrix)\n",
    "\n",
    "# 3. Обучение модели\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "model, train_acc, test_acc = train_model(model, optimizer, train_loader, test_loader, n_epoch=10)\n",
    "\n",
    "# 4. Получение accuracy на последней эпохе\n",
    "final_test_accuracy = test_acc[-1]\n",
    "print(f\"\\nFinal Test Accuracy: {final_test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77b7fd2",
   "metadata": {},
   "source": [
    "Создайте заново нейросеть Classifier.\n",
    "\n",
    "Замените веса в слое Embedding на веса из матрицы embedding_matrix.\n",
    "\n",
    "Оставьте значение для этого слоя requires_grad = True.\n",
    "\n",
    "Запустите обучение:\n",
    "\n",
    "Оптимизатор Adam со скоростью обучения 0.001.\n",
    "\n",
    "Количество эпох 10.\n",
    "\n",
    "Укажите, какая метрика accuracy получается на последней эпохе на тестовой выборке, округлив значение до второго знака после точки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "54f43fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [00:03<00:00, 92.67it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Train Loss: 0.010 - Train Acc: 0.645 Test Loss: 0.008 - Test Acc: 0.698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [00:02<00:00, 100.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 Train Loss: 0.006 - Train Acc: 0.733 Test Loss: 0.006 - Test Acc: 0.744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [00:03<00:00, 96.11it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 Train Loss: 0.005 - Train Acc: 0.769 Test Loss: 0.005 - Test Acc: 0.766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [00:02<00:00, 101.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 Train Loss: 0.004 - Train Acc: 0.793 Test Loss: 0.004 - Test Acc: 0.782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [00:03<00:00, 97.69it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 Train Loss: 0.004 - Train Acc: 0.813 Test Loss: 0.004 - Test Acc: 0.795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [00:03<00:00, 99.01it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 Train Loss: 0.004 - Train Acc: 0.830 Test Loss: 0.004 - Test Acc: 0.805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [00:03<00:00, 96.41it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 Train Loss: 0.003 - Train Acc: 0.844 Test Loss: 0.004 - Test Acc: 0.808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [00:02<00:00, 100.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 Train Loss: 0.003 - Train Acc: 0.856 Test Loss: 0.004 - Test Acc: 0.812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [00:02<00:00, 100.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 Train Loss: 0.003 - Train Acc: 0.865 Test Loss: 0.003 - Test Acc: 0.819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [00:03<00:00, 98.40it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 Train Loss: 0.003 - Train Acc: 0.872 Test Loss: 0.003 - Test Acc: 0.820\n",
      "\n",
      "Final Test Accuracy: 0.82\n"
     ]
    }
   ],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_out, embedding_matrix):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1)\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # Загружаем предобученные веса, но разрешаем их обновление\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "        self.embedding.weight.requires_grad = True  # Разрешаем обучение эмбеддингов\n",
    "        self.fc = nn.Linear(embedding_dim, n_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded_words = self.embedding(x)\n",
    "        embedded_sent = torch.mean(embedded_words, dim=1)\n",
    "        return self.fc(embedded_sent)\n",
    "\n",
    "# 2. Инициализация модели\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 100\n",
    "n_out = 6\n",
    "model = Classifier(vocab_size, embedding_dim, n_out, embedding_matrix)\n",
    "\n",
    "# 3. Обучение модели\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "model, train_acc, test_acc = train_model(model, optimizer, train_loader, test_loader, n_epoch=10)\n",
    "\n",
    "# 4. Получение accuracy на последней эпохе\n",
    "final_test_accuracy = test_acc[-1]\n",
    "print(f\"\\nFinal Test Accuracy: {final_test_accuracy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
